{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14399260,"sourceType":"datasetVersion","datasetId":9195519}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.12.11 sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:21:32.236457Z","iopub.execute_input":"2026-01-05T08:21:32.236634Z","iopub.status.idle":"2026-01-05T08:23:24.951959Z","shell.execute_reply.started":"2026-01-05T08:21:32.236615Z","shell.execute_reply":"2026-01-05T08:23:24.950907Z"}},"outputs":[{"name":"stdout","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n######################################################################## 100.0%8%############                                        48.3%#########################                                 57.6%####################################################           88.2% 88.3%##############################################################       94.4%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ollama --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:23:36.926323Z","iopub.execute_input":"2026-01-05T08:23:36.926618Z","iopub.status.idle":"2026-01-05T08:23:37.059713Z","shell.execute_reply.started":"2026-01-05T08:23:36.926587Z","shell.execute_reply":"2026-01-05T08:23:37.058997Z"}},"outputs":[{"name":"stdout","text":"Warning: could not connect to a running Ollama instance\nWarning: client version is 0.12.11\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!ollama pull dengcao/Qwen3-14B:Q5_K_M","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!echo abc","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-05T08:20:31.862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langchain langchain-community langchain-ollama chromadb pypdf langchain-chroma","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:25:55.247327Z","iopub.execute_input":"2026-01-05T08:25:55.247654Z","iopub.status.idle":"2026-01-05T08:26:18.338752Z","shell.execute_reply.started":"2026-01-05T08:25:55.247622Z","shell.execute_reply":"2026-01-05T08:26:18.337870Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\nCollecting langchain-community\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-ollama\n  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting chromadb\n  Downloading chromadb-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\nRequirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.2)\nCollecting langchain-chroma\n  Downloading langchain_chroma-1.1.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\nRequirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.37)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.5)\nCollecting SQLAlchemy<3,>=1.4 (from langchain)\n  Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-community\n  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\nINFO: pip is looking at multiple versions of langchain-ollama to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-ollama\n  Downloading langchain_ollama-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n  Downloading langchain_ollama-0.3.10-py3-none-any.whl.metadata (2.1 kB)\nCollecting ollama<1.0.0,>=0.5.3 (from langchain-ollama)\n  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\nCollecting pybase64>=1.4.1 (from chromadb)\n  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\nCollecting posthog<6.0.0,>=2.4.0 (from chromadb)\n  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\nRequirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (14.2.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\nINFO: pip is looking at multiple versions of langchain-chroma to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-chroma\n  Downloading langchain_chroma-1.0.0-py3-none-any.whl.metadata (1.9 kB)\n  Downloading langchain_chroma-0.2.6-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.6.2)\nRequirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\nRequirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\nRequirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.71.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\nCollecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\nRequirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.10.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.1rc0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nDownloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_ollama-0.3.10-py3-none-any.whl (27 kB)\nDownloading chromadb-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_chroma-0.2.6-py3-none-any.whl (12 kB)\nDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ollama-0.6.1-py3-none-any.whl (14 kB)\nDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=7c306d888bdf49de647d9cdc8767c691bc76ef81fd4ac81fafa7198732b1e9af\n  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\nSuccessfully built pypika\nInstalling collected packages: pypika, uvloop, SQLAlchemy, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, ollama, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, langchain-ollama, chromadb, langchain-community, langchain-chroma\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 1.2.19\n    Uninstalling SQLAlchemy-1.2.19:\n      Successfully uninstalled SQLAlchemy-1.2.19\n  Attempting uninstall: opentelemetry-proto\n    Found existing installation: opentelemetry-proto 1.37.0\n    Uninstalling opentelemetry-proto-1.37.0:\n      Successfully uninstalled opentelemetry-proto-1.37.0\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.37.0\n    Uninstalling opentelemetry-api-1.37.0:\n      Successfully uninstalled opentelemetry-api-1.37.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.37.0\n    Uninstalling opentelemetry-sdk-1.37.0:\n      Successfully uninstalled opentelemetry-sdk-1.37.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nxmanager 0.7.1 requires sqlalchemy==1.2.19, but you have sqlalchemy 2.0.45 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed SQLAlchemy-2.0.45 backoff-2.2.1 bcrypt-5.0.0 chromadb-1.4.0 coloredlogs-15.0.1 httptools-0.7.1 humanfriendly-10.0 langchain-chroma-0.2.6 langchain-community-0.3.31 langchain-ollama-0.3.10 mmh3-5.2.0 ollama-0.6.1 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pypika-0.48.9 uvloop-0.22.1 watchfiles-1.1.1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Install Python client (only needed once)\n!pip install ollama\n\n# Optional — upgrade to latest version\n!pip install --upgrade ollama\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:26:18.340362Z","iopub.execute_input":"2026-01-05T08:26:18.340612Z","iopub.status.idle":"2026-01-05T08:26:24.695993Z","shell.execute_reply.started":"2026-01-05T08:26:18.340587Z","shell.execute_reply":"2026-01-05T08:26:24.695329Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.1)\nRequirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\nRequirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.5)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.5)\nRequirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\nRequirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.1)\nRequirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\nRequirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.5)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.5)\nRequirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ollama_rag_chat.py – Qwen3-14B Q5_K_M via Ollama + simple RAG-style context\n# Now reading real content from gemma3.pdf instead of fake documents\n\nimport ollama\nimport time\nfrom typing import List, Tuple\n\ntry:\n    import fitz  # PyMuPDF — pip install pymupdf  (very common in Kaggle)\nexcept ImportError:\n    try:\n        from pypdf import PdfReader  # fallback\n    except ImportError:\n        raise ImportError(\"Neither PyMuPDF (fitz) nor pypdf is installed. \"\n                          \"In Kaggle: !pip install pymupdf  or  !pip install pypdf\")\n\n# ───────────────────────────────────────────────\n#  Model selection\n# ───────────────────────────────────────────────\nMODEL = \"dengcao/Qwen3-14B:Q5_K_M\"          # Community quantized Qwen3-14B (≈11 GB)\n\nPDF_PATH = \"/kaggle/input/rag-test-doc/400k.pdf\"\n\n\n# ───────────────────────────────────────────────\n#  Load PDF once at startup (simple in-memory store)\n# ───────────────────────────────────────────────\ndef load_pdf_documents(pdf_path: str) -> List[str]:\n    \"\"\"Extract text from all pages of the PDF\"\"\"\n    documents = []\n    \n    try:\n        # Preferred: PyMuPDF (fitz)\n        doc = fitz.open(pdf_path)\n        for page in doc:\n            text = page.get_text(\"text\").strip()\n            if text:\n                documents.append(text)\n        doc.close()\n        print(f\"Loaded {len(documents)} pages from {pdf_path}\")\n        \n    except NameError:\n        # Fallback: pypdf\n        reader = PdfReader(pdf_path)\n        for page in reader.pages:\n            text = page.extract_text() or \"\"\n            text = text.strip()\n            if text:\n                documents.append(text)\n        print(f\"Loaded {len(documents)} pages from {pdf_path} (using pypdf fallback)\")\n        \n    except Exception as e:\n        print(f\"Error reading PDF: {e}\")\n        return []\n    \n    return documents\n\n\n# Load once when script starts\nALL_DOCS = load_pdf_documents(PDF_PATH)\n\n\n# ───────────────────────────────────────────────\n#  Very naive \"retriever\" – top-k most relevant by length or simple keyword\n#  (in production → use sentence-transformers + FAISS/Chroma)\n# ───────────────────────────────────────────────\ndef retrieve(query: str, k: int = 4) -> List[Tuple[str, float]]:\n    \"\"\"\n    Naive retrieval: returns k longest chunks that contain any query word.\n    Replace this with real vector search in production!\n    \"\"\"\n    if not ALL_DOCS:\n        return [(\"PDF was not loaded or is empty.\", 0.99)]\n    \n    query_words = set(w.lower() for w in query.split() if len(w) > 2)\n    \n    scored_docs = []\n    for doc in ALL_DOCS:\n        doc_lower = doc.lower()\n        # very naive score = number of query words found + length bonus\n        matches = sum(1 for w in query_words if w in doc_lower)\n        score = matches * 0.4 + len(doc) / 4000.0  # normalize roughly\n        scored_docs.append((doc, min(score, 0.99)))\n    \n    # Sort by score descending\n    scored_docs.sort(key=lambda x: x[1], reverse=True)\n    \n    return scored_docs[:k]\n\n\n# ───────────────────────────────────────────────\n#  Prompt building – Qwen3 style\n# ───────────────────────────────────────────────\ndef build_prompt(query: str, k: int = 4) -> str:\n    retrieved = retrieve(query, k=k)\n    \n    context_parts = []\n    for i, (doc, score) in enumerate(retrieved, 1):\n        # Truncate very long chunks so prompt doesn't explode\n        preview = doc.replace(\"\\n\", \" \").strip()\n        if len(preview) > 800:\n            preview = preview[:750] + \" … [truncated]\"\n        context_parts.append(f\"[Doc {i} | score={score:.3f}] {preview}\")\n    \n    context_str = \"\\n\\n\".join(context_parts)\n    \n    # Qwen3-style prompt\n    prompt = f\"\"\"<|im_start|>system\nYou are a helpful assistant. Answer using the provided context when relevant.\nIf the context doesn't contain enough information, say so clearly.\nDo not make up facts.<|im_end|>\n<|im_start|>user\nContext:\n{context_str}\n\nQuestion: {query}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n    return prompt\n\n\n# ───────────────────────────────────────────────\n#  Generate with streaming + history\n# ───────────────────────────────────────────────\ndef generate_stream(query: str, max_tokens: int = 400, temperature: float = 0.7) -> str:\n    prompt = build_prompt(query)\n    \n    print(\"Prompt preview (first 800 chars):\")\n    print(prompt[:800] + \"...\" if len(prompt) > 800 else prompt)\n    print()\n    \n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    \n    try:\n        start_time = time.time()\n        \n        stream = ollama.chat(\n            model=MODEL,\n            messages=messages,\n            stream=True,\n            options={\n                \"temperature\": temperature,\n                \"top_p\": 0.9,\n                # \"num_predict\": max_tokens,   # optional\n            }\n        )\n        \n        print(\"Qwen3-14B: \", end=\"\", flush=True)\n        full_response = \"\"\n        \n        for chunk in stream:\n            content = chunk[\"message\"][\"content\"]\n            print(content, end=\"\", flush=True)\n            full_response += content\n        \n        print()  # final newline\n        \n        elapsed = time.time() - start_time\n        print(f\"({elapsed:.1f}s)\\n\")\n        \n        return full_response.strip()\n    \n    except Exception as e:\n        print(f\"\\nError during generation: {e}\")\n        print(\"Make sure Ollama is running and the model is pulled:\")\n        print(f\"  ollama pull {MODEL}\")\n        return \"\"\n\n\n# ───────────────────────────────────────────────\n#  Main – example usage\n# ───────────────────────────────────────────────\nif __name__ == \"__main__\":\n    if not ALL_DOCS:\n        print(\"Cannot continue — PDF loading failed.\")\n    else:\n        print(f\"Qwen3-14B Q5_K_M chat + RAG from gemma3.pdf via Ollama\\n\")\n        print(f\"Model : {MODEL}\")\n        print(f\"PDF   : {PDF_PATH} ({len(ALL_DOCS)} pages loaded)\")\n        print(\"Type 'exit' or 'quit' to end the script early.\\n\")\n        \n        # Optional model check\n        try:\n            models = ollama.list()\n            model_names = [m[\"name\"] for m in models.get(\"models\", [])]\n            if not any(MODEL in name for name in model_names):\n                print(f\"Warning: Model '{MODEL}' not found.\")\n                print(f\"Please run:  ollama pull {MODEL}\\n\")\n        except Exception as e:\n            print(f\"Could not check Ollama models: {e}\")\n        \n        # queries = [\n        #     \"What is the main topic of the document?\",\n        #     \"Summarize the key contributions of the paper.\",\n        #     \"What model sizes were released?\",\n        #     \"How does the license work for Gemma 3?\",\n        #     \"What is the context length of Gemma 3 models?\"\n        # ]\n\n        queries = [\n            \"Ai được hưởng 400 nghìn?\"\n        ]\n        \n        \n        for q in queries:\n            print(\"\\n\" + \"=\"*90)\n            print(f\"Query: {q}\")\n            print(\"-\"*90)\n            \n            answer = generate_stream(q, max_tokens=320, temperature=0.7)\n            print()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:59:14.926068Z","iopub.execute_input":"2026-01-05T08:59:14.926870Z","iopub.status.idle":"2026-01-05T08:59:48.409870Z","shell.execute_reply.started":"2026-01-05T08:59:14.926840Z","shell.execute_reply":"2026-01-05T08:59:48.409120Z"}},"outputs":[{"name":"stdout","text":"Loaded 6 pages from /kaggle/input/rag-test-doc/400k.pdf (using pypdf fallback)\nQwen3-14B Q5_K_M chat + RAG from gemma3.pdf via Ollama\n\nModel : dengcao/Qwen3-14B:Q5_K_M\nPDF   : /kaggle/input/rag-test-doc/400k.pdf (6 pages loaded)\nType 'exit' or 'quit' to end the script early.\n\nCould not check Ollama models: 'name'\n\n==========================================================================================\nQuery: Ai được hưởng 400 nghìn?\n------------------------------------------------------------------------------------------\nPrompt preview (first 800 chars):\n<|im_start|>system\nYou are a helpful assistant. Answer using the provided context when relevant.\nIf the context doesn't contain enough information, say so clearly.\nDo not make up facts.<|im_end|>\n<|im_start|>user\nContext:\n[Doc 1 | score=0.990] Chỉ  cần  người  dân  cung  cấp  thông  tin  tài  khoản,  mã  OTP  tài  khoản  ngân  hàng,  cài   các  app  (ứng  dụng)  lạ  trên  thiết  bị  thông  minh  sẽ  bị  chúng  chiếm  quyền  điều   khiển.   Từ   đó   tiếp   tục   dẫn   dụ   bị   hại   thao   tác   các   bước   xác   thực   nhằm   chiếm   đoạt   tiền   từ   ngân   hàng   trực   tuyến.   Để  chủ  động  phòng  ngừa,  đấu  tranh  với  các  tội  phạm  này,  Công  an  TP  Hải  Phòng   khuyến   cáo:   Người   dân   cần   nâng   cao   cảnh   giác,   không   cung   cấp   mật   khẩu,   mã   OTP   cho...\n\nQwen3-14B: <think>\nOkay, the user is asking \"Ai được hưởng 400 nghìn?\" which translates to \"Who is eligible to receive 400,000 VND?\" Let me check the provided documents to find the answer.\n\nLooking at Doc 1, it's about phishing and security advice, so probably not relevant. Doc 2 mentions Nghị quyết 418 of the Government, stating that the gift is 400,000 VND per person. It lists groups like people with merit to the revolution, social welfare recipients, and social pensioners, including orphans. Doc 3 also talks about the 400,000 VND gift during Tet 2026 for similar groups and warns about scams. Doc 4 lists specific categories like mothers of heroic mothers, war heroes, dependents of martyrs, disabled veterans, etc.\n\nSo combining Doc 2 and 3, the main groups are people with merit, social welfare recipients, social pensioners, and orphans. Doc 4 adds more specifics under those categories. The answer should include these groups, citing the documents. Also, note that if someone is in multiple categories, they only get one gift. Need to present this clearly.\n</think>\n\nTheo các tài liệu cung cấp, những đối tượng được nhận 400.000 đồng/người dịp Tết 2026 bao gồm:  \n1. **Người có công với cách mạng** (ví dụ: mẹ Việt Nam anh hùng, anh hùng lực lượng vũ trang, thương binh, người hưởng chính sách như thương binh...).  \n2. **Đối tượng bảo trợ xã hội** theo quy định hiện hành.  \n3. **Người hưởng hưu trí xã hội** (bao gồm cả trẻ mồ côi, không nơi nương tựa hoặc đang được nuôi dưỡng tại cơ sở, trại mồ côi).  \n\nNghị quyết 418 của Chính phủ quy định **mỗi người chỉ nhận một suất quà**, ngay cả khi thuộc nhiều nhóm đối tượng trên.  \n\n*Lưu ý:* Người dân cần cảnh giác với các chiêu trò lừa đảo giả danh chính sách này để chiếm đoạt thông tin cá nhân (tham khảo Doc 1 và Doc 3).\n(33.2s)\n\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ollama_rag_chat_chroma.py\n# Qwen3-14B Q5_K_M via Ollama + Chroma vector DB + sentence-transformers\n\nimport ollama\nimport time\nfrom typing import List, Tuple\n\ntry:\n    import fitz  # PyMuPDF\nexcept ImportError:\n    try:\n        from pypdf import PdfReader\n    except ImportError:\n        raise ImportError(\"Install either pymupdf or pypdf: pip install pymupdf  or  pip install pypdf\")\n\n# ─── Dependencies for proper RAG ───────────────────────────────────────\ntry:\n    import chromadb\n    from chromadb.config import Settings\n    from sentence_transformers import SentenceTransformer\nexcept ImportError:\n    raise ImportError(\n        \"Required packages missing. Please install:\\n\"\n        \"pip install chromadb sentence-transformers\"\n    )\n\n# ─── Configuration ──────────────────────────────────────────────────────\nMODEL = \"dengcao/Qwen3-14B:Q5_K_M\"           # or your favourite model\nPDF_PATH = \"/kaggle/input/rag-test-doc/400k.pdf\"\n\n# Embedding model (fast & good quality — ~80–440 MB)\nEMBEDDING_MODEL = \"all-MiniLM-L6-v2\"         # ~80 MB, fast\n# Alternatives: \"paraphrase-multilingual-MiniLM-L12-v2\" (multilingual)\n#               \"bkai-foundation-models/vietnamese-bi-encoder\" (Vietnamese)\n\nCHROMA_PATH = \"./chroma_db\"           # persistent storage\nCOLLECTION_NAME = \"400k_doc\"\n\nCHUNK_SIZE = 600\nCHUNK_OVERLAP = 120\n\n# ─── 1. Load & chunk PDF ────────────────────────────────────────────────\ndef extract_and_chunk_pdf(pdf_path: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n    \"\"\"Extract text from PDF and split into overlapping chunks\"\"\"\n    full_text = []\n\n    # Try PyMuPDF first\n    try:\n        doc = fitz.open(pdf_path)\n        for page in doc:\n            text = page.get_text(\"text\").strip()\n            if text:\n                full_text.append(text)\n        doc.close()\n        print(f\"Loaded {len(full_text)} pages (PyMuPDF)\")\n    except Exception:\n        # Fallback pypdf\n        reader = PdfReader(pdf_path)\n        for page in reader.pages:\n            text = page.extract_text() or \"\"\n            text = text.strip()\n            if text:\n                full_text.append(text)\n        print(f\"Loaded {len(full_text)} pages (pypdf fallback)\")\n\n    if not full_text:\n        raise ValueError(\"Could not extract any text from PDF\")\n\n    full_text_str = \"\\n\\n\".join(full_text)\n\n    # Naive but reasonable chunking\n    chunks = []\n    start = 0\n    while start < len(full_text_str):\n        end = start + chunk_size\n        if end > len(full_text_str):\n            end = len(full_text_str)\n        # Try to cut at paragraph or sentence boundary\n        if end < len(full_text_str):\n            while end > start and full_text_str[end] not in \"\\n.?!\":\n                end -= 1\n        chunk = full_text_str[start:end].strip()\n        if chunk:\n            chunks.append(chunk)\n        start = end - overlap if end - overlap > start else end\n\n    print(f\"Created {len(chunks)} overlapping chunks (size≈{chunk_size}, overlap={overlap})\")\n    return chunks\n\n\n# ─── 2. Build / load Chroma collection ──────────────────────────────────\ndef get_or_create_collection():\n    client = chromadb.PersistentClient(path=CHROMA_PATH)\n\n    embedding_function = SentenceTransformer(EMBEDDING_MODEL)\n    \n    # Chroma wants a callable that takes list of strings → list of embeddings\n    def embed_func(texts: List[str]) -> List[List[float]]:\n        return embedding_function.encode(texts, normalize_embeddings=True).tolist()\n\n    try:\n        collection = client.get_collection(name=COLLECTION_NAME)\n        print(f\"Loaded existing collection '{COLLECTION_NAME}' ({collection.count()} items)\")\n    except:\n        print(f\"Creating new collection '{COLLECTION_NAME}' ...\")\n        collection = client.create_collection(\n            name=COLLECTION_NAME,\n            embedding_function=embed_func,\n            metadata={\"hnsw:space\": \"cosine\"}   # or \"l2\", \"ip\"\n        )\n\n        # Load & index documents\n        chunks = extract_and_chunk_pdf(PDF_PATH)\n        if not chunks:\n            raise RuntimeError(\"No chunks created from PDF\")\n\n        # Add to Chroma (batched)\n        ids = [f\"chunk_{i:04d}\" for i in range(len(chunks))]\n        collection.add(\n            documents=chunks,\n            ids=ids,\n            metadatas=[{\"source\": \"gemma3.pdf\", \"chunk_idx\": i} for i in range(len(chunks))]\n        )\n        print(f\"Indexed {len(chunks)} chunks into Chroma\")\n\n    return collection, embedding_function\n\n\n# Global (load once)\nCOLLECTION, EMBEDDING_MODEL_OBJ = get_or_create_collection()\n\n\n# ─── 3. Retrieve ────────────────────────────────────────────────────────\ndef retrieve(query: str, k: int = 5) -> List[Tuple[str, float]]:\n    \"\"\"Semantic search using Chroma + sentence-transformers\"\"\"\n    results = COLLECTION.query(\n        query_texts=[query],\n        n_results=k,\n        include=[\"documents\", \"distances\", \"metadatas\"]\n    )\n\n    hits = []\n    for doc, dist, meta in zip(results[\"documents\"][0], results[\"distances\"][0], results[\"metadatas\"][0]):\n        # Chroma returns cosine distance → convert to similarity if needed\n        similarity = 1.0 - dist  # cosine distance → cosine similarity\n        hits.append((doc, similarity, meta))\n\n    return hits\n\n\n# ─── 4. Prompt building ─────────────────────────────────────────────────\ndef build_prompt(query: str, k: int = 5) -> str:\n    retrieved = retrieve(query, k=k)\n\n    context_parts = []\n    for i, (text, score, meta) in enumerate(retrieved, 1):\n        preview = text.replace(\"\\n\", \" \").strip()\n        if len(preview) > 900:\n            preview = preview[:850] + \"… [truncated]\"\n        source_info = f\"chunk {meta.get('chunk_idx','?')}\"\n        context_parts.append(f\"[Doc {i} | score={score:.3f} | {source_info}] {preview}\")\n\n    context_str = \"\\n\\n\".join(context_parts)\n\n    prompt = f\"\"\"<|im_start|>system\nBạn là trợ lý AI hữu ích. Hãy trả lời dựa trên ngữ cảnh được cung cấp nếu có thông tin liên quan.\nNếu ngữ cảnh không đủ để trả lời, hãy nói rõ ràng. Không bịa thông tin.<|im_end|>\n<|im_start|>user\nNgữ cảnh:\n{context_str}\n\nCâu hỏi: {query}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n    return prompt\n\n\n# ─── 5. Generate with streaming ─────────────────────────────────────────\ndef generate_stream(query: str, max_tokens: int = 500, temperature: float = 0.7) -> str:\n    prompt = build_prompt(query)\n\n    print(\"┌──────────────────────────────────────────────────────────────┐\")\n    print(\"│ Prompt preview (first 1000 chars)                            │\")\n    print(\"└──────────────────────────────────────────────────────────────┘\")\n    print(prompt[:1000] + \"…\" if len(prompt) > 1000 else prompt)\n    print()\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    try:\n        start_time = time.time()\n\n        stream = ollama.chat(\n            model=MODEL,\n            messages=messages,\n            stream=True,\n            options={\n                \"temperature\": temperature,\n                \"top_p\": 0.92,\n                # \"num_predict\": max_tokens,\n            }\n        )\n\n        print(\"Qwen3-14B: \", end=\"\", flush=True)\n        full_response = \"\"\n\n        for chunk in stream:\n            content = chunk[\"message\"][\"content\"]\n            print(content, end=\"\", flush=True)\n            full_response += content\n\n        print()  # newline\n        elapsed = time.time() - start_time\n        print(f\"({elapsed:.1f}s | {len(full_response.split())} tokens est.)\\n\")\n\n        return full_response.strip()\n\n    except Exception as e:\n        print(f\"\\nError: {e}\")\n        print(f\"Make sure Ollama is running →  ollama pull {MODEL}\")\n        return \"\"\n\n\n# ─── Main ────────────────────────────────────────────────────────────────\nif __name__ == \"__main__\":\n    print(f\"Qwen3-14B + Chroma RAG from '{PDF_PATH}'\")\n    print(f\"Embedding: {EMBEDDING_MODEL}\")\n    print(f\"Collection: {COLLECTION_NAME} ({COLLECTION.count()} chunks)\\n\")\n\n    # Example Vietnamese question from your original code\n    queries = [\n        \"Tiêu đề bài viết là gì?\",\n        \"Ai được hưởng 400 nghìn từ chính sách của nhà nước?\",\n        \"Tóm tắt nội dung chính của tài liệu\",\n        \"Gemma 3 có những kích thước mô hình nào?\",\n        \"Context length của Gemma 3 là bao nhiêu?\",\n    ]\n\n    for q in queries:\n        print(\"\\n\" + \"═\" * 80)\n        print(f\"Query: {q}\")\n        print(\"─\" * 80)\n        generate_stream(q, temperature=0.65)\n        print()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}