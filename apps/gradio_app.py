# apps/gradio_app.py
import gradio as gr
import os
import sys
import argparse

# Append the project root to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.my_rag.config import (
    LLM_MODELS,
    DEFAULT_LLM,
    EMBEDDING_MODELS,
    DEFAULT_EMBEDDING,
    hyperparams,
    VECTOR_DB_DEFAULT,
)
from src.my_rag.rag_system import RAGSystem

# Initialise the RAG system (loads Ollama models on start)
rag_system = RAGSystem(LLM_MODELS)

# Load custom CSS
CSS_PATH = os.path.join(os.path.dirname(__file__), "static", "gradio_app.css")
if os.path.exists(CSS_PATH):
    with open(CSS_PATH, "r", encoding="utf-8") as f:
        custom_css = f.read()
else:
    custom_css = ""  # fallback if file not found


def chat_with_docs(
    files,
    message,
    llm_model,
    embedding_model,
    vector_db,
    history,
    temperature,
    top_k,
    top_p,
    repeat_penalty,
    max_tokens,
    retrieval_k,
):
    """
    Main chat function called on every message or file upload.
    """
    if not message.strip():
        yield history, ""
        return

    file_paths = [f.name for f in files] if files else []

    # Re-index if new files are uploaded
    if file_paths:
        rag_system.get_or_create_collection(
            embedding_model_name=embedding_model,
            uploaded_files=file_paths,
            vector_db_type=vector_db.lower(),
        )

    # Append user message to history
    history = history + [[message, ""]] if history else [[message, ""]]

    # IMPORTANT: Update LLM model and parameters dynamically
    # We rebuild the LLM and chain with new parameters each time
    rag_system.select_llm_model(
        model_name=llm_model,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        repeat_penalty=repeat_penalty,
        max_tokens=max_tokens if max_tokens > 0 else None,
    )

    # Get retriever with desired k
    retriever = rag_system.vector_store.as_retriever(search_kwargs={"k": retrieval_k})

    # Build input for chain
    input_data = {"input": message}  # Common key; adjust if your chain uses "question"

    # Stream the response
    try:
        chain = rag_system.chain
        accumulated = ""
        for chunk in chain.stream(input_data):
            if isinstance(chunk, dict):
                text = chunk.get("answer") or chunk.get("output") or chunk.get("response") or ""
            else:
                text = str(chunk)
            accumulated += text
            history[-1][1] = accumulated
            yield history, ""
    except Exception as e:
        error_msg = f"Error during generation: {str(e)}"
        history[-1][1] = error_msg
        yield history, ""

    yield history, ""


def update_status(files):
    """Update the status markdown showing uploaded documents."""
    if not files:
        return "<div class='status'>ğŸ“­ ChÆ°a cÃ³ tÃ i liá»‡u nÃ o</div>"
    names = [os.path.basename(f.name) for f in files]
    truncated = ", ".join(names[:4]) + ("..." if len(names) > 4 else "")
    return f"<div class='status'>ğŸ“š ÄÃ£ táº£i {len(names)} tÃ i liá»‡u: {truncated}</div>"


def create_demo():
    with gr.Blocks(css=custom_css, theme=gr.themes.Soft(), title="RAG Chatbot Pro") as demo:
        gr.HTML(
            """
            <h1>ğŸ¤– RAG Chatbot Pro</h1>
            <p class='markdown'>Táº£i lÃªn tÃ i liá»‡u (PDF, DOCX, XLSX, TXT) vÃ  há»i báº¥t ká»³ cÃ¢u gÃ¬ vá» ná»™i dung cá»§a chÃºng!</p>
            """
        )

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ CÃ i Ä‘áº·t cÆ¡ báº£n")

                file_upload = gr.File(
                    label="ğŸ“‚ Táº£i lÃªn tÃ i liá»‡u",
                    file_count="multiple",
                    file_types=[".pdf", ".docx", ".xlsx", ".xls", ".txt"],
                    type="filepath",
                )

                llm_dropdown = gr.Dropdown(
                    choices=LLM_MODELS,
                    value=DEFAULT_LLM,
                    label="ğŸ§  Model LLM",
                    info="ÄÃ£ tá»± Ä‘á»™ng táº£i náº¿u cáº§n",
                )

                emb_dropdown = gr.Dropdown(
                    choices=EMBEDDING_MODELS,
                    value=DEFAULT_EMBEDDING,
                    label="ğŸ” Embedding Model",
                    info="Thay Ä‘á»•i sáº½ rebuild index",
                )

                vector_db_dropdown = gr.Dropdown(
                    choices=["chroma", "milvus", "pgvector"],
                    value=VECTOR_DB_DEFAULT,
                    label="ğŸ—„ï¸ Vector Database",
                    info="Chá»n cÆ¡ sá»Ÿ dá»¯ liá»‡u vector",
                )

                gr.Markdown("### âš™ï¸ CÃ i Ä‘áº·t nÃ¢ng cao")

                retrieval_k = gr.Slider(
                    minimum=hyperparams["retrieval"]["min_k"],
                    maximum=hyperparams["retrieval"]["max_k"],
                    value=hyperparams["retrieval"]["default_k"],
                    step=1,
                    label="ğŸ”¢ Top K Retrieval (sá»‘ chunk)",
                )

                with gr.Accordion("Generation Parameters (Ollama)", open=False):
                    temperature = gr.Slider(0.0, 2.0, value=hyperparams["generation"]["temperature"], step=0.05, label="ğŸŒ¡ï¸ Temperature")
                    top_k_slider = gr.Slider(1, 100, value=hyperparams["generation"]["top_k"], step=1, label="ğŸ” Top K")
                    top_p = gr.Slider(0.0, 1.0, value=hyperparams["generation"]["top_p"], step=0.01, label="ğŸ“ˆ Top P")
                    repeat_penalty = gr.Slider(0.0, 2.0, value=hyperparams["generation"]["repeat_penalty"], step=0.05, label="ğŸ” Repeat Penalty")
                    max_tokens = gr.Number(value=hyperparams["generation"]["max_tokens"], label="ğŸ“ Max Tokens (-1 = khÃ´ng giá»›i háº¡n)")

                status = gr.Markdown("<div class='status'>ğŸ“­ ChÆ°a cÃ³ tÃ i liá»‡u nÃ o</div>")

            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    height=620,
                    avatar_images=("ğŸ¤“", "ğŸ¤–"),
                    bubble_full_width=False,
                    show_label=False,
                )

                with gr.Row():
                    msg = gr.Textbox(
                        label="ğŸ’¬ Nháº­p cÃ¢u há»i",
                        placeholder="VÃ­ dá»¥: TÃ³m táº¯t ná»™i dung chÃ­nh cá»§a tÃ i liá»‡u?",
                        scale=5,
                        container=False,
                    )
                    send_btn = gr.Button("ğŸš€ Gá»­i", variant="primary", scale=1)

        # Events
        file_upload.change(update_status, inputs=file_upload, outputs=status)

        send_btn.click(
            chat_with_docs,
            inputs=[
                file_upload,
                msg,
                llm_dropdown,
                emb_dropdown,
                vector_db_dropdown,
                chatbot,
                temperature,
                top_k_slider,
                top_p,
                repeat_penalty,
                max_tokens,
                retrieval_k,
            ],
            outputs=[chatbot, msg],
        )

        msg.submit(
            chat_with_docs,
            inputs=[
                file_upload,
                msg,
                llm_dropdown,
                emb_dropdown,
                vector_db_dropdown,
                chatbot,
                temperature,
                top_k_slider,
                top_p,
                repeat_penalty,
                max_tokens,
                retrieval_k,
            ],
            outputs=[chatbot, msg],
        )

        gr.Markdown("### ğŸ’¡ Gá»£i Ã½ cÃ¢u há»i")
        gr.Examples(
            examples=[
                ["TÃ³m táº¯t ná»™i dung chÃ­nh cá»§a tÃ i liá»‡u"],
                ["Summary documents in English?"],
                ["Nhá»¯ng Ä‘iá»ƒm chÃ­nh trong bÃ¡o cÃ¡o lÃ  gÃ¬?"],
            ],
            inputs=msg,
        )

    return demo


def parse_args():
    parser = argparse.ArgumentParser(description="Launch RAG Chatbot Pro Gradio App")
    parser.add_argument("--share", action="store_true", help="Create a public share link (e.g., for Colab/Kaggle)")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    parser.add_argument("--port", type=int, default=7860, help="Port to run the server on")
    parser.add_argument("--server-name", type=str, default="127.0.0.1", help="Server name (use 0.0.0.0 for external access)")
    parser.add_argument("--auth", nargs=2, metavar=('username', 'password'), help="Enable basic auth: --auth username password")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    demo = create_demo()

    auth = None
    if args.auth:
        auth = (args.auth[0], args.auth[1])

    demo.launch(
        share=args.share,
        debug=args.debug,
        server_port=args.port,
        server_name=args.server_name,
        auth=auth,
    )