{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23413952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from pymongo import MongoClient\n",
    "from pymongo.operations import SearchIndexModel\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\", \"mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority\")\n",
    "DB_NAME = \"rag_db\"\n",
    "COLLECTION_NAME = \"documents\"\n",
    "EMBEDDING_MODEL = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "LLM_MODEL = \"gemma3:12b-it-qat\"\n",
    "EMBEDDING_DIM = 768  # Dimension for gte-multilingual-base\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "# Function to create vector search index (run once)\n",
    "def create_vector_search_index():\n",
    "    index_model = SearchIndexModel(\n",
    "        definition={\n",
    "            \"mappings\": {\n",
    "                \"dynamic\": True,\n",
    "                \"fields\": {\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"knnVector\",\n",
    "                        \"dimensions\": EMBEDDING_DIM,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        name=\"vector_index\"\n",
    "    )\n",
    "    collection.create_search_index(index_model)\n",
    "    print(\"Vector search index created.\")\n",
    "\n",
    "# Function to generate embeddings using Ollama\n",
    "def generate_embedding(text):\n",
    "    response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)\n",
    "    return response['embedding']\n",
    "\n",
    "# Function to ingest documents\n",
    "def ingest_documents(documents):\n",
    "    for doc in documents:\n",
    "        embedding = generate_embedding(doc['content'])\n",
    "        doc['embedding'] = embedding\n",
    "        collection.insert_one(doc)\n",
    "    print(f\"Ingested {len(documents)} documents.\")\n",
    "\n",
    "# Function to perform vector search\n",
    "def vector_search(query, top_k=5):\n",
    "    query_embedding = generate_embedding(query)\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"numCandidates\": 100,\n",
    "                \"limit\": top_k\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"content\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    return results\n",
    "\n",
    "# Function to generate RAG response\n",
    "def rag_query(query):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = vector_search(query)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs])\n",
    "    \n",
    "    # Construct prompt\n",
    "    prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer the question based on the context provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call LLM using Ollama\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment to create index (run once)\n",
    "    # create_vector_search_index()\n",
    "    \n",
    "    # Sample documents\n",
    "    sample_docs = [\n",
    "        {\"content\": \"MongoDB is a NoSQL database that supports vector search for AI applications.\"},\n",
    "        {\"content\": \"Ollama is a tool for running large language models locally.\"},\n",
    "        {\"content\": \"RAG stands for Retrieval Augmented Generation, enhancing LLMs with external knowledge.\"}\n",
    "    ]\n",
    "    \n",
    "    # Ingest sample documents (run once)\n",
    "    # ingest_documents(sample_docs)\n",
    "    \n",
    "    # Query example\n",
    "    query = \"What is RAG?\"\n",
    "    response = rag_query(query)\n",
    "    print(\"RAG Response:\")\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
