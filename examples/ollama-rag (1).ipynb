{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T09:53:02.959468Z",
     "iopub.status.busy": "2026-01-05T09:53:02.958902Z",
     "iopub.status.idle": "2026-01-05T09:53:43.244441Z",
     "shell.execute_reply": "2026-01-05T09:53:43.243777Z",
     "shell.execute_reply.started": "2026-01-05T09:53:02.959439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh  sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T09:53:47.199348Z",
     "iopub.status.busy": "2026-01-05T09:53:47.199038Z",
     "iopub.status.idle": "2026-01-05T09:53:47.330880Z",
     "shell.execute_reply": "2026-01-05T09:53:47.330159Z",
     "shell.execute_reply.started": "2026-01-05T09:53:47.199317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: could not connect to a running Ollama instance\n",
      "Warning: client version is 0.12.11\n"
     ]
    }
   ],
   "source": [
    "!ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T09:44:46.316777Z",
     "iopub.status.busy": "2026-01-05T09:44:46.316519Z",
     "iopub.status.idle": "2026-01-05T09:44:46.433586Z",
     "shell.execute_reply": "2026-01-05T09:44:46.432797Z",
     "shell.execute_reply.started": "2026-01-05T09:44:46.316739Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: ollama: command not found\n"
     ]
    }
   ],
   "source": [
    "# !ollama pull dengcao/Qwen3-14B:Q5_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ollama pull gemma3:12b-it-q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2026-01-05T10:06:39.088586Z",
     "shell.execute_reply": "2026-01-05T10:06:39.087758Z",
     "shell.execute_reply.started": "2026-01-05T10:05:29.982908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ollama pull gemma3:12b-it-qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community langchain-ollama chromadb pypdf langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T09:58:20.732045Z",
     "iopub.status.busy": "2026-01-05T09:58:20.731451Z",
     "iopub.status.idle": "2026-01-05T09:58:26.926465Z",
     "shell.execute_reply": "2026-01-05T09:58:26.925547Z",
     "shell.execute_reply.started": "2026-01-05T09:58:20.732013Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "# Install Python client (only needed once)\n",
    "!pip install ollama\n",
    "\n",
    "# Optional — upgrade to latest version\n",
    "!pip install --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-05T10:09:24.212953Z",
     "iopub.status.busy": "2026-01-05T10:09:24.212406Z",
     "iopub.status.idle": "2026-01-05T10:09:39.684474Z",
     "shell.execute_reply": "2026-01-05T10:09:39.683797Z",
     "shell.execute_reply.started": "2026-01-05T10:09:24.212926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 pages from /kaggle/input/rag-test-doc/400k.pdf (using pypdf fallback)\n",
      "Qwen3-14B Q5_K_M chat + RAG from gemma3.pdf via Ollama\n",
      "\n",
      "Model : gemma3:12b-it-qat\n",
      "PDF   : /kaggle/input/rag-test-doc/400k.pdf (6 pages loaded)\n",
      "Type 'exit' or 'quit' to end the script early.\n",
      "\n",
      "Could not check Ollama models: 'name'\n",
      "\n",
      "==========================================================================================\n",
      "Query: Ai được hưởng 400 nghìn?\n",
      "------------------------------------------------------------------------------------------\n",
      "Prompt preview (first 800 chars):\n",
      "<|im_start|>system\n",
      "Bạn là chuyên gia trợ giúp trả lời câu hỏi bằng tiếng VIệt, hảy lời các câu hỏi của người dùng và \n",
      "trả kết quả về tiếng Việt, không sử dụng tiếng Anh hoặc tiếng Trung<|im_end|>\n",
      "<|im_start|>user\n",
      "Context:\n",
      "[Doc 1 | score=0.990] Chỉ  cần  người  dân  cung  cấp  thông  tin  tài  khoản,  mã  OTP  tài  khoản  ngân  hàng,  cài   các  app  (ứng  dụng)  lạ  trên  thiết  bị  thông  minh  sẽ  bị  chúng  chiếm  quyền  điều   khiển.   Từ   đó   tiếp   tục   dẫn   dụ   bị   hại   thao   tác   các   bước   xác   thực   nhằm   chiếm   đoạt   tiền   từ   ngân   hàng   trực   tuyến.   Để  chủ  động  phòng  ngừa,  đấu  tranh  với  các  tội  phạm  này,  Công  an  TP  Hải  Phòng   khuyến   cáo:   Người   dân   cần   nâng   cao   cảnh   giác,   không   cung   cấp   mật   khẩu,   mã   OTP   cho...\n",
      "\n",
      "Qwen3-14B: Theo các tài liệu được cung cấp, những người sau đây có thể được hưởng 400.000 đồng:\n",
      "\n",
      "*   **Người có công với cách mạng:** Bao gồm cả người có công trực tiếp và thân nhân của họ.\n",
      "*   **Người hưởng hưu trí xã hội:**\n",
      "*   **Trẻ mồ côi, không nơi nương tựa hoặc đang được nuôi dưỡng tại các cơ sở, trại mồ côi.**\n",
      "*   **Thương binh, người hưởng chính sách như thương binh, thương binh loại B, bệnh binh có tỉ lệ tổn thương cơ thể từ 81% trở lên.**\n",
      "*   **Người hoạt động cách mạng bị thương.**\n",
      "*   **Bà mẹ Việt Nam anh hùng đang hưởng trợ cấp ưu đãi hàng tháng.**\n",
      "*   **Anh hùng Lực lượng vũ trang nhân dân và Anh hùng Lao động trong thời kỳ kháng chiến đang hưởng trợ cấp ưu đãi hàng tháng.**\n",
      "*   **Thân nhân liệt sĩ đang hưởng trợ cấp tuất nuôi dưỡng hàng tháng và thân nhân của từ hai liệt sĩ trở lên đang hưởng trợ cấp tuất hàng tháng.**\n",
      "*   **Người có công giúp đỡ cách mạng đang hưởng trợ cấp nuôi dưỡng hàng tháng.**\n",
      "\n",
      "Lưu ý: Nếu một người thuộc hai nhóm đối tượng trở lên, chỉ được nhận một suất quà duy nhất.\n",
      "<|im_end|>\n",
      "(15.2s)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ollama_rag_chat.py – Qwen3-14B Q5_K_M via Ollama + simple RAG-style context\n",
    "# Now reading real content from gemma3.pdf instead of fake documents\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF — pip install pymupdf  (very common in Kaggle)\n",
    "except ImportError:\n",
    "    try:\n",
    "        from pypdf import PdfReader  # fallback\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Neither PyMuPDF (fitz) nor pypdf is installed. \"\n",
    "                          \"In Kaggle: !pip install pymupdf  or  !pip install pypdf\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Model selection\n",
    "# ───────────────────────────────────────────────\n",
    "# MODEL = \"dengcao/Qwen3-14B:Q5_K_M\"          # Community quantized Qwen3-14B (≈11 GB)\n",
    "# MODEL = \"gemma3:12b-it-q8_0\"\n",
    "MODEL = \"gemma3:12b-it-qat\"\n",
    "PDF_PATH = \"/kaggle/input/rag-test-doc/400k.pdf\"\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Load PDF once at startup (simple in-memory store)\n",
    "# ───────────────────────────────────────────────\n",
    "def load_pdf_documents(pdf_path: str) -> List[str]:\n",
    "    \"\"\"Extract text from all pages of the PDF\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    try:\n",
    "        # Preferred: PyMuPDF (fitz)\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text = page.get_text(\"text\").strip()\n",
    "            if text:\n",
    "                documents.append(text)\n",
    "        doc.close()\n",
    "        print(f\"Loaded {len(documents)} pages from {pdf_path}\")\n",
    "        \n",
    "    except NameError:\n",
    "        # Fallback: pypdf\n",
    "        reader = PdfReader(pdf_path)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            text = text.strip()\n",
    "            if text:\n",
    "                documents.append(text)\n",
    "        print(f\"Loaded {len(documents)} pages from {pdf_path} (using pypdf fallback)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "# Load once when script starts\n",
    "ALL_DOCS = load_pdf_documents(PDF_PATH)\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Very naive \"retriever\" – top-k most relevant by length or simple keyword\n",
    "#  (in production → use sentence-transformers + FAISS/Chroma)\n",
    "# ───────────────────────────────────────────────\n",
    "def retrieve(query: str, k: int = 4) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Naive retrieval: returns k longest chunks that contain any query word.\n",
    "    Replace this with real vector search in production!\n",
    "    \"\"\"\n",
    "    if not ALL_DOCS:\n",
    "        return [(\"PDF was not loaded or is empty.\", 0.99)]\n",
    "    \n",
    "    query_words = set(w.lower() for w in query.split() if len(w) > 2)\n",
    "    \n",
    "    scored_docs = []\n",
    "    for doc in ALL_DOCS:\n",
    "        doc_lower = doc.lower()\n",
    "        # very naive score = number of query words found + length bonus\n",
    "        matches = sum(1 for w in query_words if w in doc_lower)\n",
    "        score = matches * 0.4 + len(doc) / 4000.0  # normalize roughly\n",
    "        scored_docs.append((doc, min(score, 0.99)))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scored_docs[:k]\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Prompt building – Qwen3 style\n",
    "# ───────────────────────────────────────────────\n",
    "def build_prompt(query: str, k: int = 4) -> str:\n",
    "    retrieved = retrieve(query, k=k)\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, (doc, score) in enumerate(retrieved, 1):\n",
    "        # Truncate very long chunks so prompt doesn't explode\n",
    "        preview = doc.replace(\"\\n\", \" \").strip()\n",
    "        if len(preview) > 800:\n",
    "            preview = preview[:750] + \" … [truncated]\"\n",
    "        context_parts.append(f\"[Doc {i} | score={score:.3f}] {preview}\")\n",
    "    \n",
    "    context_str = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Qwen3-style prompt\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "Bạn là chuyên gia trợ giúp trả lời câu hỏi bằng tiếng VIệt, hảy lời các câu hỏi của người dùng và \n",
    "trả kết quả về tiếng Việt, không sử dụng tiếng Anh hoặc tiếng Trung<|im_end|>\n",
    "<|im_start|>user\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Generate with streaming + history\n",
    "# ───────────────────────────────────────────────\n",
    "def generate_stream(query: str, max_tokens: int = 400, temperature: float = 0.7) -> str:\n",
    "    prompt = build_prompt(query)\n",
    "    \n",
    "    print(\"Prompt preview (first 800 chars):\")\n",
    "    print(prompt[:800] + \"...\" if len(prompt) > 800 else prompt)\n",
    "    print()\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        stream = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            options={\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": 0.9,\n",
    "                # \"num_predict\": max_tokens,   # optional\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"Qwen3-14B: \", end=\"\", flush=True)\n",
    "        full_response = \"\"\n",
    "        \n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "        \n",
    "        print()  # final newline\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"({elapsed:.1f}s)\\n\")\n",
    "        \n",
    "        return full_response.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during generation: {e}\")\n",
    "        print(\"Make sure Ollama is running and the model is pulled:\")\n",
    "        print(f\"  ollama pull {MODEL}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Main – example usage\n",
    "# ───────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    if not ALL_DOCS:\n",
    "        print(\"Cannot continue — PDF loading failed.\")\n",
    "    else:\n",
    "        print(f\"Qwen3-14B Q5_K_M chat + RAG from gemma3.pdf via Ollama\\n\")\n",
    "        print(f\"Model : {MODEL}\")\n",
    "        print(f\"PDF   : {PDF_PATH} ({len(ALL_DOCS)} pages loaded)\")\n",
    "        print(\"Type 'exit' or 'quit' to end the script early.\\n\")\n",
    "        \n",
    "        # Optional model check\n",
    "        try:\n",
    "            models = ollama.list()\n",
    "            model_names = [m[\"name\"] for m in models.get(\"models\", [])]\n",
    "            if not any(MODEL in name for name in model_names):\n",
    "                print(f\"Warning: Model '{MODEL}' not found.\")\n",
    "                print(f\"Please run:  ollama pull {MODEL}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not check Ollama models: {e}\")\n",
    "        \n",
    "        # queries = [\n",
    "        #     \"What is the main topic of the document?\",\n",
    "        #     \"Summarize the key contributions of the paper.\",\n",
    "        #     \"What model sizes were released?\",\n",
    "        #     \"How does the license work for Gemma 3?\",\n",
    "        #     \"What is the context length of Gemma 3 models?\"\n",
    "        # ]\n",
    "\n",
    "        queries = [\n",
    "            \"Ai được hưởng 400 nghìn?\"\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        for q in queries:\n",
    "            print(\"\\n\" + \"=\"*90)\n",
    "            print(f\"Query: {q}\")\n",
    "            print(\"-\"*90)\n",
    "            \n",
    "            answer = generate_stream(q, max_tokens=320, temperature=0.7)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T10:14:56.735233Z",
     "iopub.status.busy": "2026-01-05T10:14:56.734503Z",
     "iopub.status.idle": "2026-01-05T10:15:39.131157Z",
     "shell.execute_reply": "2026-01-05T10:15:39.130374Z",
     "shell.execute_reply.started": "2026-01-05T10:14:56.735202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new collection 'doc_400k' ...\n",
      "Loaded 6 pages using pypdf fallback\n",
      "Created 14 overlapping chunks (≈600 chars, overlap=120)\n",
      "Indexed 14 chunks into Chroma\n",
      "RAG pipeline ready • Model: gemma3:12b-it-qat\n",
      "Embedding: all-MiniLM-L6-v2\n",
      "Collection: doc_400k (14 chunks total)\n",
      "\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "Query: Tiêu đề bài viết là gì?\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "┌──────────────────────────────────────────────────────────────┐\n",
      "│ Prompt preview (first 1400 chars)                            │\n",
      "└──────────────────────────────────────────────────────────────┘\n",
      "<|im_start|>system\n",
      "Bạn là trợ lý AI hữu ích. Hãy trả lời ngắn gọn, chính xác dựa trên ngữ cảnh được cung cấp.\n",
      "Nếu không có thông tin liên quan trong ngữ cảnh, hãy nói rõ \"Không tìm thấy thông tin liên quan trong tài liệu\".\n",
      "Không bịa thông tin, không suy diễn quá mức.<|im_end|>\n",
      "<|im_start|>user\n",
      "Ngữ cảnh:\n",
      "[Doc 1 | score=0.624 | chunk 4] thiết  bị  thông  minh  sẽ  bị  chúng  chiếm  quyền  điều   khiển.   Từ   đó   tiếp   tục   dẫn   dụ   bị   hại   thao   tác   các   bước   xác   thực   nhằm   chiếm   đoạt   tiền   từ   ngân   hàng   trực   tuyến.   Để  chủ  động  phòng  ngừa,  đấu  tranh  với  các  tội  phạm  này,  Công  an  TP  Hải  Phòng   khuyến   cáo:   Người   dân   cần   nâng   cao   cảnh   giác,   không   cung   cấp   mật   khẩu,   mã   OTP   cho   bất   kỳ   ai.   Không  bấm  vào  các  đường  link  lạ,  không  cài  đặt  các  ứng  dụng  không  rõ  nguồn   gốc   đòi   hỏi   quyền   truy   cập   cao   vào   hệ   thống\n",
      "\n",
      "[Doc 2 | score=0.618 | chunk 3] hức  của  người  dân,  gửi  cho  họ  các   đường   link   có   chứa   mã   độc   thông   qua   các   tin   nhắn   hoặc   cuộc   gọi   mạo   danh   là   cán   bộ   của   cơ   quan   nhà   nước   hỗ   trợ   người   dân   nhận   quà.        Đảng,  Nhà  nước  tặng  quà  nhân  dịp  chào  mừng  Đại  hội  Đảng  XIV  và  Tết   Nguyên  đán    ĐỌC  NGAY  Chỉ  cần  người  dân  cung  cấp  thông  tin  tài  khoản,  mã  OTP  tài  khoản  ngân …\n",
      "\n",
      "gemma3:12b-it-qat: Không tìm thấy thông tin liên quan trong tài liệu.\n",
      "<|im_end|>\n",
      "(10.2s | ~11 tokens)\n",
      "\n",
      "\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "Query: Ai được hưởng 400 nghìn từ chính sách của nhà nước?\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "┌──────────────────────────────────────────────────────────────┐\n",
      "│ Prompt preview (first 1400 chars)                            │\n",
      "└──────────────────────────────────────────────────────────────┘\n",
      "<|im_start|>system\n",
      "Bạn là trợ lý AI hữu ích. Hãy trả lời ngắn gọn, chính xác dựa trên ngữ cảnh được cung cấp.\n",
      "Nếu không có thông tin liên quan trong ngữ cảnh, hãy nói rõ \"Không tìm thấy thông tin liên quan trong tài liệu\".\n",
      "Không bịa thông tin, không suy diễn quá mức.<|im_end|>\n",
      "<|im_start|>user\n",
      "Ngữ cảnh:\n",
      "[Doc 1 | score=0.711 | chunk 6] c.   Khi  nghi  ngờ  bị  lừa  đảo  chiếm  đoạt  tài  sản,  người  dân  cần  báo  ngay  cho  cơ  quan   công   an   nơi   gần   nhất   để   được   hỗ   trợ   giải   quyết.   Những  ai  sẽ  được  nhận  quà  400.000  đồng?  Còn  theo  nghị  quyết  418  của  Chính  phủ,  mức  quà  tặng  áp  dụng  thống  nhất  là   400.000   đồng/người.   Theo  quy  định  tại  nghị  quyết,  ngoài  người  có  công  với  cách  mạng,  những  người   được   tặng   quà   còn   gồm   các   đối   tượng   bảo   trợ   xã   hội   theo   quy   định   hiện   hành;   người   hưởng   hưu   trí   xã   hội   theo   nghị   định\n",
      "\n",
      "[Doc 2 | score=0.702 | chunk 2] ân  dịp  chào  mừng  Đại  hội  XIV  của  Đảng,  bầu  cử  Quốc  hội   khóa   XVI   và   Tết   Nguyên   đán   Bính   Ngọ   năm   2026   cho   người   có   công   với   cách   mạng,   người   nhận   hưu   trí   xã   hội   và   đối   tượng   yếu   thế   khác.   Tuy  nhiên  lợi  dụng  chính  sách  ý  nghĩa  này,  một  số  đối  tượng  xấu  đã  tung  ra  nhiều   thủ   đoạn   lừa   đảo   tinh   vi   nhằm   chiếm   đoạt   tài   sản.   Các…\n",
      "\n",
      "gemma3:12b-it-qat: Người có công, thân nhân người có công, người nhận hưu trí xã hội và đối tượng yếu thế được nhận 400.000 đồng của Đảng, Nhà nước dịp Tết 2026.\n",
      "<|im_end|>\n",
      "(6.9s | ~31 tokens)\n",
      "\n",
      "\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "Query: Tóm tắt nội dung chính của tài liệu\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "┌──────────────────────────────────────────────────────────────┐\n",
      "│ Prompt preview (first 1400 chars)                            │\n",
      "└──────────────────────────────────────────────────────────────┘\n",
      "<|im_start|>system\n",
      "Bạn là trợ lý AI hữu ích. Hãy trả lời ngắn gọn, chính xác dựa trên ngữ cảnh được cung cấp.\n",
      "Nếu không có thông tin liên quan trong ngữ cảnh, hãy nói rõ \"Không tìm thấy thông tin liên quan trong tài liệu\".\n",
      "Không bịa thông tin, không suy diễn quá mức.<|im_end|>\n",
      "<|im_start|>user\n",
      "Ngữ cảnh:\n",
      "[Doc 1 | score=0.580 | chunk 2] ân  dịp  chào  mừng  Đại  hội  XIV  của  Đảng,  bầu  cử  Quốc  hội   khóa   XVI   và   Tết   Nguyên   đán   Bính   Ngọ   năm   2026   cho   người   có   công   với   cách   mạng,   người   nhận   hưu   trí   xã   hội   và   đối   tượng   yếu   thế   khác.   Tuy  nhiên  lợi  dụng  chính  sách  ý  nghĩa  này,  một  số  đối  tượng  xấu  đã  tung  ra  nhiều   thủ   đoạn   lừa   đảo   tinh   vi   nhằm   chiếm   đoạt   tài   sản.   Các  đối  tượng  thường  nhắm  vào  tâm  lý  háo  hức  của  người  dân,  gửi  cho  họ  các   đường   link   có   chứa   mã   độc   thông   qua   các   tin   nhắn   hoặc\n",
      "\n",
      "[Doc 2 | score=0.563 | chunk 3] hức  của  người  dân,  gửi  cho  họ  các   đường   link   có   chứa   mã   độc   thông   qua   các   tin   nhắn   hoặc   cuộc   gọi   mạo   danh   là   cán   bộ   của   cơ   quan   nhà   nước   hỗ   trợ   người   dân   nhận   quà.        Đảng,  Nhà  nước  tặng  quà  nhân  dịp  chào  mừng  Đại  hội  Đảng  XIV  và  Tết   Nguyên  đán    ĐỌC  NGAY  Chỉ  cần  người  dân  cung  cấp  thông  tin  tài  khoản,  mã  OTP  tài  khoản  ngân …\n",
      "\n",
      "gemma3:12b-it-qat: Nội dung chính của tài liệu là về việc cảnh báo người dân về các chiêu trò lừa đảo liên quan đến việc nhận quà của Đảng, Nhà nước nhân dịp Đại hội XIV của Đảng, bầu cử Quốc hội khóa XVI và Tết Nguyên đán Bính Ngọ năm 2026. Kẻ lừa đảo thường lợi dụng tâm lý háo hức của người dân, gửi đường link độc hại qua tin nhắn hoặc cuộc gọi mạo danh cán bộ nhà nước để chiếm đoạt tài sản. Người dân cần cảnh giác, không cung cấp thông tin tài khoản, mã OTP, không cài đặt ứng dụng lạ và cần báo cáo ngay cho cơ quan công an khi nghi ngờ bị lừa đảo. Chính phủ đã ban hành nghị quyết về việc tặng quà cho một số nhóm đối tượng trong xã hội, và việc nhận quà cần thông qua các kênh chính thống.\n",
      "<|im_end|>\n",
      "(11.9s | ~153 tokens)\n",
      "\n",
      "\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "Query: Gemma 3 có những kích thước mô hình nào?\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "┌──────────────────────────────────────────────────────────────┐\n",
      "│ Prompt preview (first 1400 chars)                            │\n",
      "└──────────────────────────────────────────────────────────────┘\n",
      "<|im_start|>system\n",
      "Bạn là trợ lý AI hữu ích. Hãy trả lời ngắn gọn, chính xác dựa trên ngữ cảnh được cung cấp.\n",
      "Nếu không có thông tin liên quan trong ngữ cảnh, hãy nói rõ \"Không tìm thấy thông tin liên quan trong tài liệu\".\n",
      "Không bịa thông tin, không suy diễn quá mức.<|im_end|>\n",
      "<|im_start|>user\n",
      "Ngữ cảnh:\n",
      "[Doc 1 | score=0.484 | chunk 7] bảo   trợ   xã   hội   theo   quy   định   hiện   hành;   người   hưởng   hưu   trí   xã   hội   theo   nghị   định   mới   của   Chính   phủ,   bao   gồm   cả   trẻ   mồ   côi,   không   nơi   nương   tựa   hoặc   đang   được   nuôi   dưỡng   tại   các   cơ   sở,   trại   mồ   côi.   Trường  hợp  một  người  được  xác  nhận  thuộc  từ  hai  nhóm  đối  tượng  trở  lên,  đủ   điều   kiện   hưởng   quà,   nghị   quyết   quy   định   chỉ   nhận   một   suất   quà,   không   cộng   dồn   nhiều   mức.   Nghị  quyết  nêu  rõ  việc  tặng  quà  hoàn  thành  chậm  nhất  trong  ngày  31-12-2025.\n",
      "\n",
      "[Doc 2 | score=0.451 | chunk 1] tâm,  cả  về   tinh   thần,   vật   chất,   sức   khỏe,   chế   độ   chính   sách   -   Ảnh   minh   họa:   HÀ   QUÂN  Công  an  TP  Hải  Phòng vừa  có  cảnh  báo  lừa  đảo  liên  quan  đến  việc  nhận  tiền   400.000  đồng  dịp  Tết  2026  do  Đảng,  Nhà  nước  tặng  cho  một  số  nhóm  người   trong   xã   hội.   Cảnh  giác  với  chiêu  trò  lừa  đảo   Theo  cơ  quan  công  an,  Chính  phủ đã  ban  hành  nghị  quyết  418  về  việc …\n",
      "\n",
      "gemma3:12b-it-qat: Không tìm thấy thông tin liên quan trong tài liệu.\n",
      "<|im_end|>\n",
      "\n",
      "(5.8s | ~11 tokens)\n",
      "\n",
      "\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "Query: Context length của Gemma 3 là bao nhiêu?\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "┌──────────────────────────────────────────────────────────────┐\n",
      "│ Prompt preview (first 1400 chars)                            │\n",
      "└──────────────────────────────────────────────────────────────┘\n",
      "<|im_start|>system\n",
      "Bạn là trợ lý AI hữu ích. Hãy trả lời ngắn gọn, chính xác dựa trên ngữ cảnh được cung cấp.\n",
      "Nếu không có thông tin liên quan trong ngữ cảnh, hãy nói rõ \"Không tìm thấy thông tin liên quan trong tài liệu\".\n",
      "Không bịa thông tin, không suy diễn quá mức.<|im_end|>\n",
      "<|im_start|>user\n",
      "Ngữ cảnh:\n",
      "[Doc 1 | score=0.516 | chunk 2] ân  dịp  chào  mừng  Đại  hội  XIV  của  Đảng,  bầu  cử  Quốc  hội   khóa   XVI   và   Tết   Nguyên   đán   Bính   Ngọ   năm   2026   cho   người   có   công   với   cách   mạng,   người   nhận   hưu   trí   xã   hội   và   đối   tượng   yếu   thế   khác.   Tuy  nhiên  lợi  dụng  chính  sách  ý  nghĩa  này,  một  số  đối  tượng  xấu  đã  tung  ra  nhiều   thủ   đoạn   lừa   đảo   tinh   vi   nhằm   chiếm   đoạt   tài   sản.   Các  đối  tượng  thường  nhắm  vào  tâm  lý  háo  hức  của  người  dân,  gửi  cho  họ  các   đường   link   có   chứa   mã   độc   thông   qua   các   tin   nhắn   hoặc\n",
      "\n",
      "[Doc 2 | score=0.492 | chunk 3] hức  của  người  dân,  gửi  cho  họ  các   đường   link   có   chứa   mã   độc   thông   qua   các   tin   nhắn   hoặc   cuộc   gọi   mạo   danh   là   cán   bộ   của   cơ   quan   nhà   nước   hỗ   trợ   người   dân   nhận   quà.        Đảng,  Nhà  nước  tặng  quà  nhân  dịp  chào  mừng  Đại  hội  Đảng  XIV  và  Tết   Nguyên  đán    ĐỌC  NGAY  Chỉ  cần  người  dân  cung  cấp  thông  tin  tài  khoản,  mã  OTP  tài  khoản  ngân …\n",
      "\n",
      "gemma3:12b-it-qat: Không tìm thấy thông tin liên quan trong tài liệu.\n",
      "\n",
      "(5.4s | ~10 tokens)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ollama_rag_chat_chroma_fixed.py\n",
    "# Fixed version - works with modern ChromaDB (0.4.16+)\n",
    "# Using Qwen / Gemma via Ollama + Chroma + sentence-transformers\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    try:\n",
    "        from pypdf import PdfReader\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Install either pymupdf or pypdf: pip install pymupdf or pip install pypdf\")\n",
    "\n",
    "# ─── Core dependencies ──────────────────────────────────────────────────\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.utils import embedding_functions\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Required packages missing. Install:\\n\"\n",
    "        \"pip install chromadb sentence-transformers\"\n",
    "    )\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────\n",
    "MODEL = \"gemma3:12b-it-qat\"              # or \"qwen2:7b\", \"llama3.1:8b\", etc.\n",
    "# MODEL = \"dengcao/Qwen3-14B:Q5_K_M\"     # if you have this variant\n",
    "\n",
    "PDF_PATH = \"/kaggle/input/rag-test-doc/400k.pdf\"\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"     # fast & good (~80 MB)\n",
    "# Alternatives:\n",
    "#   \"paraphrase-multilingual-MiniLM-L12-v2\"   # multilingual\n",
    "#   \"bkai-foundation-models/vietnamese-bi-encoder\"   # Vietnamese-focused\n",
    "\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"doc_400k\"\n",
    "\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 120\n",
    "\n",
    "# ─── 1. PDF → overlapping chunks ────────────────────────────────────────\n",
    "def extract_and_chunk_pdf(pdf_path: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    \"\"\"Extract text from PDF and split into overlapping chunks\"\"\"\n",
    "    full_text = []\n",
    "\n",
    "    # Prefer PyMuPDF (faster, better layout)\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text = page.get_text(\"text\").strip()\n",
    "            if text:\n",
    "                full_text.append(text)\n",
    "        doc.close()\n",
    "        print(f\"Loaded {len(full_text)} pages using PyMuPDF\")\n",
    "    except Exception:\n",
    "        # Fallback to pypdf\n",
    "        reader = PdfReader(pdf_path)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            text = text.strip()\n",
    "            if text:\n",
    "                full_text.append(text)\n",
    "        print(f\"Loaded {len(full_text)} pages using pypdf fallback\")\n",
    "\n",
    "    if not full_text:\n",
    "        raise ValueError(\"No text could be extracted from the PDF\")\n",
    "\n",
    "    full_text_str = \"\\n\\n\".join(full_text)\n",
    "\n",
    "    # Simple overlapping chunking with boundary awareness\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text_str):\n",
    "        end = min(start + chunk_size, len(full_text_str))\n",
    "        if end < len(full_text_str):\n",
    "            # Try to end at natural break (paragraph, sentence)\n",
    "            while end > start and full_text_str[end] not in \"\\n.?!\":\n",
    "                end -= 1\n",
    "        chunk = full_text_str[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        # Move start position with overlap\n",
    "        start = end - overlap if end - overlap > start else end\n",
    "\n",
    "    print(f\"Created {len(chunks)} overlapping chunks (≈{chunk_size} chars, overlap={overlap})\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ─── 2. Chroma collection (create or load) ──────────────────────────────\n",
    "def get_or_create_collection():\n",
    "    client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "    # Modern ChromaDB-compatible embedding function\n",
    "    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        collection = client.get_collection(\n",
    "            name=COLLECTION_NAME,\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "        print(f\"Loaded existing collection '{COLLECTION_NAME}' ({collection.count()} items)\")\n",
    "    except:\n",
    "        print(f\"Creating new collection '{COLLECTION_NAME}' ...\")\n",
    "\n",
    "        collection = client.create_collection(\n",
    "            name=COLLECTION_NAME,\n",
    "            embedding_function=embedding_function,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # cosine / l2 / ip\n",
    "        )\n",
    "\n",
    "        # Index the document\n",
    "        chunks = extract_and_chunk_pdf(PDF_PATH)\n",
    "        if not chunks:\n",
    "            raise RuntimeError(\"No chunks created from PDF\")\n",
    "\n",
    "        ids = [f\"chunk_{i:04d}\" for i in range(len(chunks))]\n",
    "        collection.add(\n",
    "            documents=chunks,\n",
    "            ids=ids,\n",
    "            metadatas=[{\"source\": \"400k.pdf\", \"chunk_idx\": i} for i in range(len(chunks))]\n",
    "        )\n",
    "        print(f\"Indexed {len(chunks)} chunks into Chroma\")\n",
    "\n",
    "    return collection, embedding_function\n",
    "\n",
    "\n",
    "# Global singleton (loaded once)\n",
    "COLLECTION, EMBEDDING_FN = get_or_create_collection()\n",
    "\n",
    "\n",
    "# ─── 3. Retrieve relevant chunks ────────────────────────────────────────\n",
    "def retrieve(query: str, k: int = 5) -> List[Tuple[str, float, dict]]:\n",
    "    results = COLLECTION.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    hits = []\n",
    "    for doc, dist, meta in zip(\n",
    "        results[\"documents\"][0],\n",
    "        results[\"distances\"][0],\n",
    "        results[\"metadatas\"][0]\n",
    "    ):\n",
    "        similarity = 1.0 - dist if dist is not None else 0.0  # cosine distance → similarity\n",
    "        hits.append((doc, similarity, meta))\n",
    "\n",
    "    return hits\n",
    "\n",
    "\n",
    "# ─── 4. Build prompt with context ───────────────────────────────────────\n",
    "def build_prompt(query: str, k: int = 5) -> str:\n",
    "    retrieved = retrieve(query, k=k)\n",
    "\n",
    "    context_parts = []\n",
    "    for i, (text, score, meta) in enumerate(retrieved, 1):\n",
    "        preview = text.replace(\"\\n\", \" \").strip()\n",
    "        if len(preview) > 900:\n",
    "            preview = preview[:850] + \"… [truncated]\"\n",
    "        chunk_id = meta.get('chunk_idx', '?')\n",
    "        context_parts.append(\n",
    "            f\"[Doc {i} | score={score:.3f} | chunk {chunk_id}] {preview}\"\n",
    "        )\n",
    "\n",
    "    context_str = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "Bạn là trợ lý AI hữu ích. Hãy trả lời ngắn gọn, chính xác dựa trên ngữ cảnh được cung cấp.\n",
    "Nếu không có thông tin liên quan trong ngữ cảnh, hãy nói rõ \"Không tìm thấy thông tin liên quan trong tài liệu\".\n",
    "Không bịa thông tin, không suy diễn quá mức.<|im_end|>\n",
    "<|im_start|>user\n",
    "Ngữ cảnh:\n",
    "{context_str}\n",
    "\n",
    "Câu hỏi: {query}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ─── 5. Generate answer (streaming) ─────────────────────────────────────\n",
    "def generate_stream(query: str, max_tokens: int = 600, temperature: float = 0.7) -> str:\n",
    "    prompt = build_prompt(query)\n",
    "\n",
    "    print(\"┌──────────────────────────────────────────────────────────────┐\")\n",
    "    print(\"│ Prompt preview (first 1400 chars)                            │\")\n",
    "    print(\"└──────────────────────────────────────────────────────────────┘\")\n",
    "    preview = prompt[:1400] + \"…\" if len(prompt) > 1400 else prompt\n",
    "    print(preview)\n",
    "    print()\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        stream = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            options={\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": 0.92,\n",
    "                # \"num_predict\": max_tokens,  # optional limit\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"{MODEL}: \", end=\"\", flush=True)\n",
    "        full_response = \"\"\n",
    "\n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "\n",
    "        print()  # final newline\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"({elapsed:.1f}s | ~{len(full_response.split())} tokens)\\n\")\n",
    "\n",
    "        return full_response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during generation: {e}\")\n",
    "        print(f\"Make sure Ollama is running and model is pulled:  ollama pull {MODEL}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# ─── Main ────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"RAG pipeline ready • Model: {MODEL}\")\n",
    "    print(f\"Embedding: {EMBEDDING_MODEL}\")\n",
    "    print(f\"Collection: {COLLECTION_NAME} ({COLLECTION.count()} chunks total)\\n\")\n",
    "\n",
    "    queries = [\n",
    "        \"Tiêu đề bài viết là gì?\",\n",
    "        \"Ai được hưởng 400 nghìn từ chính sách của nhà nước?\",\n",
    "        \"Tóm tắt nội dung chính của tài liệu\",\n",
    "        \"Gemma 3 có những kích thước mô hình nào?\",\n",
    "        \"Context length của Gemma 3 là bao nhiêu?\",\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        print(\"\\n\" + \"═\" * 80)\n",
    "        print(f\"Query: {q}\")\n",
    "        print(\"─\" * 80)\n",
    "        generate_stream(q, temperature=0.65)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9195519,
     "sourceId": 14399260,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
