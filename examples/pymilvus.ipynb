{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fal8zYBbt53p"
      },
      "outputs": [],
      "source": [
        "# !pip install \"pymilvus[model,milvus_lite]\" -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj_Gk_xcq0Ej",
        "outputId": "329f45d1-69e6-4be7-9ef3-d867a4ccb870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m285.1/285.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m328.2/328.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pymilvus[model]==2.6.6 milvus-lite==2.5.1 pypdf==6.4.2 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50uwyp1r87qR"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from pymilvus import MilvusClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from pypdf import PdfReader\n",
        "from typing import List\n",
        "import os\n",
        "\n",
        "# ------------------- Embedding Model -------------------\n",
        "print(\"Loading embedding model: Alibaba-NLP/gte-multilingual-base ...\")\n",
        "\n",
        "embeding_model = 'all-MiniLM-L6-v2'\n",
        "# embeding_model = 'Alibaba-NLP/gte-multilingual-base'\n",
        "\n",
        "embedding_model = SentenceTransformer(\n",
        "    embeding_model,\n",
        "    trust_remote_code=True   # ‚Üê This fixes the ValueError\n",
        ")\n",
        "DIM = embedding_model.get_sentence_embedding_dimension()  # Should be 768\n",
        "\n",
        "def embed_fn(texts: List[str], is_query: bool = False) -> List[List[float]]:\n",
        "    \"\"\"Unified embedding function for documents and queries\"\"\"\n",
        "    # gte models work significantly better with this instruction prefix for queries\n",
        "    if is_query:\n",
        "        prefixed = [f\"Represent this sentence for searching relevant passages: {t}\" for t in texts]\n",
        "    else:\n",
        "        prefixed = texts\n",
        "\n",
        "    embeddings = embedding_model.encode(\n",
        "        prefixed,\n",
        "        normalize_embeddings=True,      # Very important for cosine similarity\n",
        "        batch_size=32,\n",
        "        show_progress_bar=False,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    return embeddings.tolist()\n",
        "\n",
        "# ------------------- LLM -------------------\n",
        "# Currently one of the best reasonable local instruction-tuned models (2025/2026)\n",
        "# model_id = \"Qwen/Qwen3-1.7B\"           # ‚Üê most recommended choice\n",
        "model_id = \"google/gemma-3-4b-it\"\n",
        "# Alternatives:\n",
        "# model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "# model_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
        "\n",
        "print(f\"Loading LLM: {model_id} (this may take several minutes)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# ------------------- Milvus Lite -------------------\n",
        "DB_FILE = \"pdf_rag_local_gte.db\"           # changed name to avoid conflict with old db\n",
        "client = MilvusClient(DB_FILE)\n",
        "collection_name = \"pdf_collection\"\n",
        "\n",
        "# ------------------- Helpers -------------------\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF: {e}\"\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 1200, overlap: int = 300) -> List[str]:\n",
        "    if not text.strip():\n",
        "        return []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_len = len(text)\n",
        "    while start < text_len:\n",
        "        end = min(start + chunk_size, text_len)\n",
        "        chunk = text[start:end]\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk.strip())\n",
        "        if end >= text_len:\n",
        "            break\n",
        "        start = end - overlap\n",
        "    return [c for c in chunks if len(c) > 80]\n",
        "\n",
        "def init_collection():\n",
        "    if client.has_collection(collection_name):\n",
        "        client.drop_collection(collection_name)\n",
        "\n",
        "    client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        dimension=DIM,\n",
        "        metric_type=\"COSINE\",\n",
        "        auto_id=False,\n",
        "        enable_dynamic_field=True\n",
        "    )\n",
        "\n",
        "    # Insert sample data\n",
        "    sample_docs = [\n",
        "        \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
        "        \"The Dartmouth Conference in 1956 is considered the founding event of AI.\",\n",
        "        \"John McCarthy coined the term 'Artificial Intelligence' in 1955.\",\n",
        "        \"Alan Turing conducted pioneering research in AI and computing.\",\n",
        "        \"Machine learning is a subset of artificial intelligence focused on data-driven algorithms.\",\n",
        "    ]\n",
        "\n",
        "    vectors = embed_fn(sample_docs, is_query=False)\n",
        "    data = [\n",
        "        {\"id\": i, \"vector\": v, \"text\": d, \"source\": \"sample\"}\n",
        "        for i, (d, v) in enumerate(zip(sample_docs, vectors))\n",
        "    ]\n",
        "    client.insert(collection_name, data)\n",
        "\n",
        "    # Create index\n",
        "    index_params = client.prepare_index_params()\n",
        "    index_params.add_index(field_name=\"vector\", index_type=\"AUTOINDEX\", metric_type=\"COSINE\")\n",
        "    client.create_index(collection_name, index_params)\n",
        "    client.load_collection(collection_name)\n",
        "\n",
        "# Safe way to get all existing IDs\n",
        "def get_all_existing_ids(client, collection_name: str, batch_size: int = 8000) -> List[int]:\n",
        "    all_ids = []\n",
        "    offset = 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            res = client.query(\n",
        "                collection_name=collection_name,\n",
        "                filter=\"\",\n",
        "                output_fields=[\"id\"],\n",
        "                limit=batch_size,\n",
        "                offset=offset\n",
        "            )\n",
        "            if not res:\n",
        "                break\n",
        "\n",
        "            batch_ids = [r[\"id\"] for r in res]\n",
        "            all_ids.extend(batch_ids)\n",
        "\n",
        "            if len(res) < batch_size:\n",
        "                break\n",
        "\n",
        "            offset += batch_size\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during ID pagination: {e}\")\n",
        "            break\n",
        "\n",
        "    return all_ids\n",
        "\n",
        "# Initialize collection if needed\n",
        "if not client.has_collection(collection_name):\n",
        "    init_collection()\n",
        "else:\n",
        "    client.load_collection(collection_name)\n",
        "\n",
        "# Get next available ID\n",
        "existing_ids = get_all_existing_ids(client, collection_name)\n",
        "next_id = max(existing_ids, default=-1) + 1\n",
        "print(f\"Next available ID will start from: {next_id}\")\n",
        "\n",
        "# ------------------- PDF Indexing -------------------\n",
        "def process_pdfs(pdf_files: List[gr.File], chunk_size: int, chunk_overlap: int, progress=gr.Progress()):\n",
        "    global next_id\n",
        "    if not pdf_files:\n",
        "        return \"No PDFs uploaded.\"\n",
        "\n",
        "    # Remove previous user documents\n",
        "    client.delete(collection_name, filter=\"source like 'pdf_%'\")\n",
        "\n",
        "    progress(0, desc=\"Starting...\")\n",
        "    total_chunks = 0\n",
        "    messages = []\n",
        "\n",
        "    for idx, file_obj in enumerate(pdf_files):\n",
        "        filename = os.path.basename(file_obj.name)\n",
        "        progress((idx + 0.1) / len(pdf_files), desc=f\"Reading {filename}...\")\n",
        "        text = extract_text_from_pdf(file_obj.name)\n",
        "\n",
        "        if text.startswith(\"Error\"):\n",
        "            messages.append(f\"‚úó Failed to read: {filename}\")\n",
        "            continue\n",
        "\n",
        "        progress((idx + 0.4) / len(pdf_files), desc=f\"Chunking {filename}...\")\n",
        "        chunks = chunk_text(text, chunk_size=chunk_size, overlap=chunk_overlap)\n",
        "\n",
        "        if not chunks:\n",
        "            messages.append(f\"‚úó No useful text extracted from {filename}\")\n",
        "            continue\n",
        "\n",
        "        progress((idx + 0.7) / len(pdf_files), desc=f\"Embedding {len(chunks)} chunks...\")\n",
        "        vectors = embed_fn(chunks, is_query=False)\n",
        "\n",
        "        data = []\n",
        "        for j, (chunk, vec) in enumerate(zip(chunks, vectors)):\n",
        "            data.append({\n",
        "                \"id\": next_id,\n",
        "                \"vector\": vec,\n",
        "                \"text\": chunk,\n",
        "                \"source\": f\"pdf_{filename}_chunk_{j}\"\n",
        "            })\n",
        "            next_id += 1\n",
        "\n",
        "        progress((idx + 0.9) / len(pdf_files), desc=\"Saving to database...\")\n",
        "        res = client.insert(collection_name, data)\n",
        "        total_chunks += res[\"insert_count\"]\n",
        "        messages.append(f\"‚úì {len(chunks)} chunks from {filename}\")\n",
        "\n",
        "    return f\"**Indexing complete!**\\nTotal chunks: **{total_chunks}**\\n\\n\" + \"\\n\".join(messages)\n",
        "\n",
        "# ------------------- RAG Query -------------------\n",
        "def rag_query(question: str, top_k: int = 6, temperature: float = 0.75):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question.\", \"\"\n",
        "\n",
        "    # Use query prefix for better retrieval\n",
        "    query_vec = embed_fn([question], is_query=True)[0]\n",
        "\n",
        "    results = client.search(\n",
        "        collection_name=collection_name,\n",
        "        data=[query_vec],\n",
        "        limit=top_k,\n",
        "        output_fields=[\"text\", \"source\"],\n",
        "        search_params={\"metric_type\": \"COSINE\"}\n",
        "    )[0]\n",
        "\n",
        "    context_lines = []\n",
        "    for hit in results:\n",
        "        score = hit[\"distance\"]\n",
        "        source = hit[\"entity\"][\"source\"]\n",
        "        text = hit[\"entity\"][\"text\"]\n",
        "        preview = text[:480] + \"...\" if len(text) > 480 else text\n",
        "        context_lines.append(f\"[{score:.3f}] {source}\\n{preview}\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_lines)\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful, precise assistant. Answer the question based **only** on the provided context.\n",
        "If the information is insufficient or not present, reply only with \"I don't know.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
        "        outputs = llm.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=600,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=0.92,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer = full_output[len(prompt):].strip()\n",
        "    except Exception as e:\n",
        "        answer = f\"Generation error: {str(e)}\"\n",
        "\n",
        "    context_display = \"Retrieved context (score | source):\\n\\n\" + context\n",
        "    return answer, context_display\n",
        "\n",
        "# ------------------- Gradio Interface -------------------\n",
        "with gr.Blocks(title=\"PDF RAG ‚Ä¢ GTE-Multilingual\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# üìò Local PDF RAG Chat\\n**Embedding: gte-multilingual-base** ‚Ä¢ 100% offline\")\n",
        "\n",
        "    with gr.Tab(\"üì§ Upload & Index\"):\n",
        "        pdf_input = gr.File(\n",
        "            label=\"Upload PDF file(s)\",\n",
        "            file_count=\"multiple\",\n",
        "            file_types=[\".pdf\"],\n",
        "            type=\"filepath\"\n",
        "        )\n",
        "        with gr.Row():\n",
        "            chunk_size = gr.Slider(400, 6000, value=1200, step=100, label=\"Chunk Size (characters)\")\n",
        "            chunk_overlap = gr.Slider(50, 1500, value=300, step=50, label=\"Chunk Overlap\")\n",
        "        index_button = gr.Button(\"üöÄ Index PDFs\", variant=\"primary\", size=\"lg\")\n",
        "        status_output = gr.Textbox(label=\"Indexing Status\", lines=10, interactive=False)\n",
        "\n",
        "        index_button.click(\n",
        "            process_pdfs,\n",
        "            inputs=[pdf_input, chunk_size, chunk_overlap],\n",
        "            outputs=status_output\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"‚ùì Ask Questions\"):\n",
        "        gr.Markdown(\"Ask anything about the indexed documents (supports multilingual questions)\")\n",
        "        question_input = gr.Textbox(\n",
        "            label=\"Your question\",\n",
        "            placeholder=\"What are the main conclusions of the report? / Â†±ÂëäÁöÑ‰∏ªË¶ÅÁµêË´ñÊòØ‰ªÄÈ∫ºÔºü\",\n",
        "            lines=4\n",
        "        )\n",
        "        with gr.Row():\n",
        "            top_k_slider = gr.Slider(2, 12, value=6, step=1, label=\"Top-K chunks\")\n",
        "            temperature_slider = gr.Slider(0.1, 1.5, value=0.75, step=0.05, label=\"Temperature\")\n",
        "        ask_button = gr.Button(\"Ask\", variant=\"primary\", size=\"lg\")\n",
        "        answer_output = gr.Textbox(label=\"Answer\", lines=10)\n",
        "        context_output = gr.Textbox(label=\"Retrieved Context\", lines=12, interactive=False)\n",
        "\n",
        "        ask_button.click(\n",
        "            rag_query,\n",
        "            inputs=[question_input, top_k_slider, temperature_slider],\n",
        "            outputs=[answer_output, context_output]\n",
        "        )\n",
        "\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                \"When was artificial intelligence founded as an academic discipline?\",\n",
        "                \"What did John McCarthy contribute to AI?\",\n",
        "                \"Â†±ÂëäÁöÑ‰∏ªË¶ÅÁôºÁèæÊòØ‰ªÄÈ∫ºÔºü\",\n",
        "                \"Summarize the key findings from the uploaded document.\"\n",
        "            ],\n",
        "            inputs=question_input\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Gradio app on http://localhost:7860 (public link enabled)\")\n",
        "    demo.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7862,\n",
        "        debug=False,\n",
        "        share=True   # public temporary link\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}