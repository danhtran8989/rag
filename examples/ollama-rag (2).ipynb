{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T09:53:02.959468Z",
     "iopub.status.busy": "2026-01-05T09:53:02.958902Z",
     "iopub.status.idle": "2026-01-05T09:53:43.244441Z",
     "shell.execute_reply": "2026-01-05T09:53:43.243777Z",
     "shell.execute_reply.started": "2026-01-05T09:53:02.959439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# !curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.12.11 sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T06:57:17.519990Z",
     "iopub.status.busy": "2026-01-06T06:57:17.519523Z",
     "iopub.status.idle": "2026-01-06T06:57:17.643015Z",
     "shell.execute_reply": "2026-01-06T06:57:17.640774Z",
     "shell.execute_reply.started": "2026-01-06T06:57:17.519945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: ollama: command not found\n"
     ]
    }
   ],
   "source": [
    "!ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T09:44:46.316777Z",
     "iopub.status.busy": "2026-01-05T09:44:46.316519Z",
     "iopub.status.idle": "2026-01-05T09:44:46.433586Z",
     "shell.execute_reply": "2026-01-05T09:44:46.432797Z",
     "shell.execute_reply.started": "2026-01-05T09:44:46.316739Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: ollama: command not found\n"
     ]
    }
   ],
   "source": [
    "# !ollama pull dengcao/Qwen3-14B:Q5_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ollama pull gemma3:12b-it-q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Quantization int4\n",
    "!ollama pull gemma3:12b-it-qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community langchain-ollama chromadb pypdf langchain-chroma ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ollama_rag_chat.py – Qwen3-14B Q5_K_M via Ollama + simple RAG-style context\n",
    "# Now reading real content from gemma3.pdf instead of fake documents\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF — pip install pymupdf  (very common in Kaggle)\n",
    "except ImportError:\n",
    "    try:\n",
    "        from pypdf import PdfReader  # fallback\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Neither PyMuPDF (fitz) nor pypdf is installed. \"\n",
    "                          \"In Kaggle: !pip install pymupdf  or  !pip install pypdf\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Model selection\n",
    "# ───────────────────────────────────────────────\n",
    "# MODEL = \"dengcao/Qwen3-14B:Q5_K_M\"          # Community quantized Qwen3-14B (≈11 GB)\n",
    "# MODEL = \"gemma3:12b-it-q8_0\"\n",
    "MODEL = \"gemma3:12b-it-qat\"\n",
    "PDF_PATH = \"/kaggle/input/rag-test-doc/400k.pdf\"\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Load PDF once at startup (simple in-memory store)\n",
    "# ───────────────────────────────────────────────\n",
    "def load_pdf_documents(pdf_path: str) -> List[str]:\n",
    "    \"\"\"Extract text from all pages of the PDF\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    try:\n",
    "        # Preferred: PyMuPDF (fitz)\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text = page.get_text(\"text\").strip()\n",
    "            if text:\n",
    "                documents.append(text)\n",
    "        doc.close()\n",
    "        print(f\"Loaded {len(documents)} pages from {pdf_path}\")\n",
    "        \n",
    "    except NameError:\n",
    "        # Fallback: pypdf\n",
    "        reader = PdfReader(pdf_path)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            text = text.strip()\n",
    "            if text:\n",
    "                documents.append(text)\n",
    "        print(f\"Loaded {len(documents)} pages from {pdf_path} (using pypdf fallback)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "# Load once when script starts\n",
    "ALL_DOCS = load_pdf_documents(PDF_PATH)\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Very naive \"retriever\" – top-k most relevant by length or simple keyword\n",
    "#  (in production → use sentence-transformers + FAISS/Chroma)\n",
    "# ───────────────────────────────────────────────\n",
    "def retrieve(query: str, k: int = 4) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Naive retrieval: returns k longest chunks that contain any query word.\n",
    "    Replace this with real vector search in production!\n",
    "    \"\"\"\n",
    "    if not ALL_DOCS:\n",
    "        return [(\"PDF was not loaded or is empty.\", 0.99)]\n",
    "    \n",
    "    query_words = set(w.lower() for w in query.split() if len(w) > 2)\n",
    "    \n",
    "    scored_docs = []\n",
    "    for doc in ALL_DOCS:\n",
    "        doc_lower = doc.lower()\n",
    "        # very naive score = number of query words found + length bonus\n",
    "        matches = sum(1 for w in query_words if w in doc_lower)\n",
    "        score = matches * 0.4 + len(doc) / 4000.0  # normalize roughly\n",
    "        scored_docs.append((doc, min(score, 0.99)))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scored_docs[:k]\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Prompt building – Qwen3 style\n",
    "# ───────────────────────────────────────────────\n",
    "def build_prompt(query: str, k: int = 4) -> str:\n",
    "    retrieved = retrieve(query, k=k)\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, (doc, score) in enumerate(retrieved, 1):\n",
    "        # Truncate very long chunks so prompt doesn't explode\n",
    "        preview = doc.replace(\"\\n\", \" \").strip()\n",
    "        if len(preview) > 800:\n",
    "            preview = preview[:750] + \" … [truncated]\"\n",
    "        context_parts.append(f\"[Doc {i} | score={score:.3f}] {preview}\")\n",
    "    \n",
    "    context_str = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Qwen3-style prompt\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "Bạn là chuyên gia trợ giúp trả lời câu hỏi bằng tiếng VIệt, hảy lời các câu hỏi của người dùng và \n",
    "trả kết quả về tiếng Việt, không sử dụng tiếng Anh hoặc tiếng Trung<|im_end|>\n",
    "<|im_start|>user\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Generate with streaming + history\n",
    "# ───────────────────────────────────────────────\n",
    "def generate_stream(query: str, max_tokens: int = 400, temperature: float = 0.7) -> str:\n",
    "    prompt = build_prompt(query)\n",
    "    \n",
    "    print(\"Prompt preview (first 800 chars):\")\n",
    "    print(prompt[:800] + \"...\" if len(prompt) > 800 else prompt)\n",
    "    print()\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        stream = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            options={\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": 0.9,\n",
    "                # \"num_predict\": max_tokens,   # optional\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"Qwen3-14B: \", end=\"\", flush=True)\n",
    "        full_response = \"\"\n",
    "        \n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "        \n",
    "        print()  # final newline\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"({elapsed:.1f}s)\\n\")\n",
    "        \n",
    "        return full_response.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during generation: {e}\")\n",
    "        print(\"Make sure Ollama is running and the model is pulled:\")\n",
    "        print(f\"  ollama pull {MODEL}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "#  Main – example usage\n",
    "# ───────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    if not ALL_DOCS:\n",
    "        print(\"Cannot continue — PDF loading failed.\")\n",
    "    else:\n",
    "        print(f\"Qwen3-14B Q5_K_M chat + RAG from gemma3.pdf via Ollama\\n\")\n",
    "        print(f\"Model : {MODEL}\")\n",
    "        print(f\"PDF   : {PDF_PATH} ({len(ALL_DOCS)} pages loaded)\")\n",
    "        print(\"Type 'exit' or 'quit' to end the script early.\\n\")\n",
    "        \n",
    "        # Optional model check\n",
    "        try:\n",
    "            models = ollama.list()\n",
    "            model_names = [m[\"name\"] for m in models.get(\"models\", [])]\n",
    "            if not any(MODEL in name for name in model_names):\n",
    "                print(f\"Warning: Model '{MODEL}' not found.\")\n",
    "                print(f\"Please run:  ollama pull {MODEL}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not check Ollama models: {e}\")\n",
    "        \n",
    "        # queries = [\n",
    "        #     \"What is the main topic of the document?\",\n",
    "        #     \"Summarize the key contributions of the paper.\",\n",
    "        #     \"What model sizes were released?\",\n",
    "        #     \"How does the license work for Gemma 3?\",\n",
    "        #     \"What is the context length of Gemma 3 models?\"\n",
    "        # ]\n",
    "\n",
    "        queries = [\n",
    "            \"Ai được hưởng 400 nghìn?\"\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        for q in queries:\n",
    "            print(\"\\n\" + \"=\"*90)\n",
    "            print(f\"Query: {q}\")\n",
    "            print(\"-\"*90)\n",
    "            \n",
    "            answer = generate_stream(q, max_tokens=320, temperature=0.7)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ollama_rag_chat_chroma_fixed.py\n",
    "# Fixed version - works with modern ChromaDB (0.4.16+)\n",
    "# Using Qwen / Gemma via Ollama + Chroma + sentence-transformers\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    try:\n",
    "        from pypdf import PdfReader\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Install either pymupdf or pypdf: pip install pymupdf or pip install pypdf\")\n",
    "\n",
    "# ─── Core dependencies ──────────────────────────────────────────────────\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.utils import embedding_functions\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Required packages missing. Install:\\n\"\n",
    "        \"pip install chromadb sentence-transformers\"\n",
    "    )\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────\n",
    "MODEL = \"qwen3:14b-Q4_K_M\"\n",
    "MODEL = \"gemma3:12b-it-qat\"\n",
    "MODEL = \"llama4:17b-scout-16e-instruct-q4_K_M\"\n",
    "MODEL = \"deepseek-r1:14b-qwen-distill-q4_K_M\"\n",
    "# PDF_PATH = \"/kaggle/input/rag-test-doc/400k.pdf\"\n",
    "\n",
    "\n",
    "PDF_PATH = \"/kaggle/input/rag-test-doc/400k.pdf\"\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"     # fast & good (~80 MB)\n",
    "# Alternatives:\n",
    "#   \"paraphrase-multilingual-MiniLM-L12-v2\"   # multilingual\n",
    "#   \"bkai-foundation-models/vietnamese-bi-encoder\"   # Vietnamese-focused\n",
    "\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"doc_400k\"\n",
    "\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 120\n",
    "\n",
    "# ─── 1. PDF → overlapping chunks ────────────────────────────────────────\n",
    "def extract_and_chunk_pdf(pdf_path: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    \"\"\"Extract text from PDF and split into overlapping chunks\"\"\"\n",
    "    full_text = []\n",
    "\n",
    "    # Prefer PyMuPDF (faster, better layout)\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text = page.get_text(\"text\").strip()\n",
    "            if text:\n",
    "                full_text.append(text)\n",
    "        doc.close()\n",
    "        print(f\"Loaded {len(full_text)} pages using PyMuPDF\")\n",
    "    except Exception:\n",
    "        # Fallback to pypdf\n",
    "        reader = PdfReader(pdf_path)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            text = text.strip()\n",
    "            if text:\n",
    "                full_text.append(text)\n",
    "        print(f\"Loaded {len(full_text)} pages using pypdf fallback\")\n",
    "\n",
    "    if not full_text:\n",
    "        raise ValueError(\"No text could be extracted from the PDF\")\n",
    "\n",
    "    full_text_str = \"\\n\\n\".join(full_text)\n",
    "\n",
    "    # Simple overlapping chunking with boundary awareness\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text_str):\n",
    "        end = min(start + chunk_size, len(full_text_str))\n",
    "        if end < len(full_text_str):\n",
    "            # Try to end at natural break (paragraph, sentence)\n",
    "            while end > start and full_text_str[end] not in \"\\n.?!\":\n",
    "                end -= 1\n",
    "        chunk = full_text_str[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        # Move start position with overlap\n",
    "        start = end - overlap if end - overlap > start else end\n",
    "\n",
    "    print(f\"Created {len(chunks)} overlapping chunks (≈{chunk_size} chars, overlap={overlap})\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ─── 2. Chroma collection (create or load) ──────────────────────────────\n",
    "def get_or_create_collection():\n",
    "    client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "    # Modern ChromaDB-compatible embedding function\n",
    "    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        collection = client.get_collection(\n",
    "            name=COLLECTION_NAME,\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "        print(f\"Loaded existing collection '{COLLECTION_NAME}' ({collection.count()} items)\")\n",
    "    except:\n",
    "        print(f\"Creating new collection '{COLLECTION_NAME}' ...\")\n",
    "\n",
    "        collection = client.create_collection(\n",
    "            name=COLLECTION_NAME,\n",
    "            embedding_function=embedding_function,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # cosine / l2 / ip\n",
    "        )\n",
    "\n",
    "        # Index the document\n",
    "        chunks = extract_and_chunk_pdf(PDF_PATH)\n",
    "        if not chunks:\n",
    "            raise RuntimeError(\"No chunks created from PDF\")\n",
    "\n",
    "        ids = [f\"chunk_{i:04d}\" for i in range(len(chunks))]\n",
    "        collection.add(\n",
    "            documents=chunks,\n",
    "            ids=ids,\n",
    "            metadatas=[{\"source\": \"400k.pdf\", \"chunk_idx\": i} for i in range(len(chunks))]\n",
    "        )\n",
    "        print(f\"Indexed {len(chunks)} chunks into Chroma\")\n",
    "\n",
    "    return collection, embedding_function\n",
    "\n",
    "\n",
    "# Global singleton (loaded once)\n",
    "COLLECTION, EMBEDDING_FN = get_or_create_collection()\n",
    "\n",
    "\n",
    "# ─── 3. Retrieve relevant chunks ────────────────────────────────────────\n",
    "def retrieve(query: str, k: int = 5) -> List[Tuple[str, float, dict]]:\n",
    "    results = COLLECTION.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    hits = []\n",
    "    for doc, dist, meta in zip(\n",
    "        results[\"documents\"][0],\n",
    "        results[\"distances\"][0],\n",
    "        results[\"metadatas\"][0]\n",
    "    ):\n",
    "        similarity = 1.0 - dist if dist is not None else 0.0  # cosine distance → similarity\n",
    "        hits.append((doc, similarity, meta))\n",
    "\n",
    "    return hits\n",
    "\n",
    "\n",
    "# ─── 4. Build prompt with context ───────────────────────────────────────\n",
    "def build_prompt(query: str, k: int = 5) -> str:\n",
    "    retrieved = retrieve(query, k=k)\n",
    "\n",
    "    context_parts = []\n",
    "    for i, (text, score, meta) in enumerate(retrieved, 1):\n",
    "        preview = text.replace(\"\\n\", \" \").strip()\n",
    "        if len(preview) > 900:\n",
    "            preview = preview[:850] + \"… [truncated]\"\n",
    "        chunk_id = meta.get('chunk_idx', '?')\n",
    "        context_parts.append(\n",
    "            f\"[Doc {i} | score={score:.3f} | chunk {chunk_id}] {preview}\"\n",
    "        )\n",
    "\n",
    "    context_str = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "Bạn là trợ lý AI hữu ích. Hãy trả lời ngắn gọn, chính xác dựa trên ngữ cảnh được cung cấp.\n",
    "Nếu không có thông tin liên quan trong ngữ cảnh, hãy nói rõ \"Không tìm thấy thông tin liên quan trong tài liệu\".\n",
    "Không bịa thông tin, không suy diễn quá mức.<|im_end|>\n",
    "<|im_start|>user\n",
    "Ngữ cảnh:\n",
    "{context_str}\n",
    "\n",
    "Câu hỏi: {query}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ─── 5. Generate answer (streaming) ─────────────────────────────────────\n",
    "def generate_stream(query: str, max_tokens: int = 600, temperature: float = 0.7) -> str:\n",
    "    prompt = build_prompt(query)\n",
    "\n",
    "    print(\"┌──────────────────────────────────────────────────────────────┐\")\n",
    "    print(\"│ Prompt preview (first 1400 chars)                            │\")\n",
    "    print(\"└──────────────────────────────────────────────────────────────┘\")\n",
    "    preview = prompt[:1400] + \"…\" if len(prompt) > 1400 else prompt\n",
    "    print(preview)\n",
    "    print()\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        stream = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            options={\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": 0.92,\n",
    "                # \"num_predict\": max_tokens,  # optional limit\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"{MODEL}: \", end=\"\", flush=True)\n",
    "        full_response = \"\"\n",
    "\n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "\n",
    "        print()  # final newline\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"({elapsed:.1f}s | ~{len(full_response.split())} tokens)\\n\")\n",
    "\n",
    "        return full_response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during generation: {e}\")\n",
    "        print(f\"Make sure Ollama is running and model is pulled:  ollama pull {MODEL}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# ─── Main ────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"RAG pipeline ready • Model: {MODEL}\")\n",
    "    print(f\"Embedding: {EMBEDDING_MODEL}\")\n",
    "    print(f\"Collection: {COLLECTION_NAME} ({COLLECTION.count()} chunks total)\\n\")\n",
    "\n",
    "    queries = [\n",
    "        \"Tiêu đề bài viết là gì?\",\n",
    "        \"Ai được hưởng 400 nghìn từ chính sách của nhà nước?\",\n",
    "        \"Tóm tắt nội dung chính của tài liệu\",\n",
    "        \"Gemma 3 có những kích thước mô hình nào?\",\n",
    "        \"Context length của Gemma 3 là bao nhiêu?\",\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        print(\"\\n\" + \"═\" * 80)\n",
    "        print(f\"Query: {q}\")\n",
    "        print(\"─\" * 80)\n",
    "        generate_stream(q, temperature=0.65)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9195519,
     "sourceId": 14399260,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
