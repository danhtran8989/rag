# # src/my_rag/rag_system.py

# import torch
# from chromadb.utils import embedding_functions
# from typing import List, Tuple, Generator, Dict
# import hashlib
# import os
# import ollama

# from .config import (
#     CHUNK_SIZE,
#     CHUNK_OVERLAP,
#     VECTOR_DB_DEFAULT,
#     VECTOR_DB_CONFIG
# )
# from .text_extraction import extract_text
# from .chunking import chunk_text
# from .utils import ensure_ollama_models
# from .vector_stores import get_vector_store


# def get_file_hash(file_path: str, chunk_size: int = 8192) -> str:
#     """T√≠nh hash MD5 c·ªßa file ƒë·ªÉ ph√°t hi·ªán thay ƒë·ªïi n·ªôi dung."""
#     md5 = hashlib.md5()
#     with open(file_path, "rb") as f:
#         for block in iter(lambda: f.read(chunk_size), b""):
#             md5.update(block)
#     return md5.hexdigest()


# class RAGSystem:
#     """
#     Singleton RAG System ƒë·ªÉ ƒë·∫£m b·∫£o vector_store v√† tr·∫°ng th√°i index
#     ƒë∆∞·ª£c gi·ªØ nguy√™n gi·ªØa c√°c l·∫ßn g·ªçi (r·∫•t quan tr·ªçng trong Streamlit, Gradio, FastAPI...).
#     """
#     _instance = None
#     _initialized = False

#     def __new__(cls, llm_models: List[str] = None):
#         if cls._instance is None:
#             cls._instance = super(RAGSystem, cls).__new__(cls)
#         return cls._instance

#     def __init__(self, llm_models: List[str] = None):
#         # Ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn duy nh·∫•t
#         if not RAGSystem._initialized:
#             if llm_models:
#                 ensure_ollama_models(llm_models)

#             self.vector_store = None
#             self.embedding_fn = None
#             self.current_embedding_model = None

#             # L∆∞u tr·ªØ hash c·ªßa c√°c file ƒë√£ index: {file_path: hash}
#             self.indexed_files: Dict[str, str] = {}

#             self.store_type = vector_db_type or VECTOR_DB_DEFAULT
#             self.store_type = VECTOR_DB_DEFAULT
#             self.llm_model = None
#             self.gen_params = {}

#             config = VECTOR_DB_CONFIG[self.store_type]
#             self.vector_store = get_vector_store(self.store_type, **config)

#             RAGSystem._initialized = True
#             print("‚úÖ RAGSystem singleton instance created and initialized.")

#     def _get_embedding_fn(self, embedding_model_name: str):
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         if embedding_model_name != self.current_embedding_model or self.embedding_fn is None:
#             self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
#                 model_name=embedding_model_name,
#                 device=device,
#                 normalize_embeddings=True,
#                 trust_remote_code=True
#             )
#             self.current_embedding_model = embedding_model_name
#         return self.embedding_fn

#     def get_or_create_collection(
#         self,
#         embedding_model_name: str,
#         uploaded_files: List[str],
#         vector_db_type: str = None,
#     ):
#         """T·∫°o ho·∫∑c t√°i s·ª≠ d·ª•ng collection d·ª±a tr√™n thay ƒë·ªïi file."""
        
#         embedding_fn = self._get_embedding_fn(embedding_model_name)
        
        

#         # T√≠nh hash hi·ªán t·∫°i c·ªßa c√°c file
#         current_hashes: Dict[str, str] = {}
#         valid_files = []
#         for file_path in (uploaded_files or []):
#             if os.path.exists(file_path):
#                 file_hash = get_file_hash(file_path)
#                 current_hashes[file_path] = file_hash
#                 valid_files.append(file_path)
#             else:
#                 print(f"‚ö†Ô∏è File kh√¥ng t·ªìn t·∫°i (c√≥ th·ªÉ ƒë√£ b·ªã x√≥a): {file_path}")

#         print(f"Indexed files (before): {self.indexed_files}")
#         print(f"Current hashes: {current_hashes}")

#         # Ki·ªÉm tra thay ƒë·ªïi
#         files_changed = False

#         # File m·ªõi ho·∫∑c thay ƒë·ªïi n·ªôi dung
#         for fp, new_hash in current_hashes.items():
#             old_hash = self.indexed_files.get(fp)
#             if old_hash != new_hash:
#                 files_changed = True
#                 print(f"üìÑ File thay ƒë·ªïi ho·∫∑c m·ªõi: {os.path.basename(fp)}")

#         # File b·ªã x√≥a kh·ªèi danh s√°ch
#         for old_fp in list(self.indexed_files.keys()):
#             if old_fp not in current_hashes:
#                 files_changed = True
#                 print(f"üóëÔ∏è File b·ªã x√≥a kh·ªèi danh s√°ch: {os.path.basename(old_fp)}")

#         # N·∫øu c√≥ thay ƒë·ªïi ho·∫∑c collection r·ªóng ‚Üí rebuild
#         if files_changed or self.vector_store.count() == 0:
#             print("üîÑ Ph√°t hi·ªán thay ƒë·ªïi ho·∫∑c collection r·ªóng ‚Üí Rebuild to√†n b·ªô...")
#             self.vector_store.delete_collection()
#             self.vector_store.get_or_create_collection(
#                 embedding_fn=embedding_fn,
#                 collection_name=config["collection_name"]
#             )

#             chunks, ids, metadatas = [], [], []
#             for file_path in valid_files:
#                 filename = os.path.basename(file_path)
#                 print(f"üìÑ ƒêang x·ª≠ l√Ω: {filename}")
#                 text = extract_text(file_path)
#                 for i, chunk in enumerate(chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)):
#                     chunk_id = f"{filename}_chunk_{i:04d}"
#                     chunks.append(chunk)
#                     ids.append(chunk_id)
#                     metadatas.append({"source": file_path})

#             if chunks:
#                 self.vector_store.add_documents(chunks, ids, metadatas)
#                 print(f"‚úÖ ƒê√£ index {len(chunks)} chunks v√†o {self.store_type.upper()}")

#             # C·∫≠p nh·∫≠t tr·∫°ng th√°i index
#             self.indexed_files = current_hashes.copy()
#         else:
#             print("‚úÖ Kh√¥ng c√≥ thay ƒë·ªïi ‚Üí Gi·ªØ nguy√™n collection hi·ªán t·∫°i.")

#     def retrieve(self, query: str, k: int = 6) -> List[Tuple[str, float, dict]]:
#         """Retrieve c√°c chunk li√™n quan nh·∫•t."""
#         if not self.vector_store or self.vector_store.count() == 0:
#             print("‚ö†Ô∏è Vector store ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o ho·∫∑c r·ªóng.")
#             return []

#         results = self.vector_store.query(query, self.embedding_fn, k=k)
#         if not results or not results.get("documents"):
#             return []

#         return [
#             (doc, 1.0 - (dist or 0), meta)
#             for doc, dist, meta in zip(
#                 results["documents"][0],
#                 results["distances"][0],
#                 results["metadatas"][0]
#             )
#         ]

#     def build_prompt(self, query: str, context_items: List[Tuple[str, float, dict]]) -> str:
#         """X√¢y d·ª±ng prompt c√≥ ng·ªØ c·∫£nh."""
#         context_text = "\n\n".join([
#             f"[Ngu·ªìn: {os.path.basename(m['source'])}]: {c}"
#             for c, s, m in context_items
#         ])
#         prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh v√† ch√≠nh x√°c. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a CH·ªà v√†o th√¥ng tin ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y.
# N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan trong ng·ªØ c·∫£nh, h√£y tr·∫£ l·ªùi "T√¥i kh√¥ng bi·∫øt" ho·∫∑c "Th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu".

# NG·ªÆ C·∫¢NH:
# {context_text}

# C√ÇU H·ªéI: {query}
# TR·∫¢ L·ªúI:"""
#         return prompt

#     def stream_answer(
#         self,
#         query: str,
#         k: int = 6,
#         model: str = "llama3",
#         params: dict = None
#     ) -> Generator[str, None, None]:
#         """Stream c√¢u tr·∫£ l·ªùi t·ª´ Ollama v·ªõi RAG."""
#         context = self.retrieve(query, k=k)
#         prompt = self.build_prompt(query, context)

#         options = {
#             "temperature": params.get("temperature", 0.7) if params else 0.7,
#             "top_k": params.get("top_k", 40) if params else 40,
#             "top_p": params.get("top_p", 0.9) if params else 0.9,
#             "repeat_penalty": params.get("repeat_penalty", 1.1) if params else 1.1,
#         }
#         if params and params.get("max_tokens") and params["max_tokens"] > 0:
#             options["num_predict"] = params["max_tokens"]

#         stream = ollama.chat(
#             model=model,
#             messages=[{"role": "user", "content": prompt}],
#             stream=True,
#             options=options,
#         )

#         for chunk in stream:
#             yield chunk["message"]["content"]   

# src/my_rag/rag_system.py
import torch
from chromadb.utils import embedding_functions
from typing import List, Tuple, Generator, Dict
import hashlib
import os
import ollama

from .config import (
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    VECTOR_DB_DEFAULT,
    VECTOR_DB_CONFIG
)
from .text_extraction import extract_text
from .chunking import chunk_text
from .utils import ensure_ollama_models
from .vector_stores import get_vector_store


def get_file_hash(file_path: str, chunk_size: int = 8192) -> str:
    """T√≠nh hash SHA-256 c·ªßa file ƒë·ªÉ ph√°t hi·ªán thay ƒë·ªïi n·ªôi dung."""
    sha256 = hashlib.sha256()
    with open(file_path, "rb") as f:
        for block in iter(lambda: f.read(chunk_size), b""):
            sha256.update(block)
    return sha256.hexdigest()


class RAGSystem:
    """
    Singleton RAG System ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ c√≥ m·ªôt instance duy nh·∫•t,
    gi·ªØ tr·∫°ng th√°i vector store v√† indexed files gi·ªØa c√°c request.
    """
    _instance = None
    _initialized = False

    def __new__(cls, llm_models: List[str] = None):
        if cls._instance is None:
            cls._instance = super(RAGSystem, cls).__new__(cls)
        return cls._instance

    def __init__(self, llm_models: List[str] = None):
        if not RAGSystem._initialized:
            if llm_models:
                ensure_ollama_models(llm_models)

            # Kh·ªüi t·∫°o c√°c thu·ªôc t√≠nh c∆° b·∫£n
            self.vector_store = None
            self.embedding_fn = None
            self.current_embedding_model = None
            self.indexed_files: Dict[str, str] = {}  # {file_path: hash}
            self.store_type = VECTOR_DB_DEFAULT

            # T·∫°o vector_store ngay t·ª´ ƒë·∫ßu v·ªõi config m·∫∑c ƒë·ªãnh
            config = VECTOR_DB_CONFIG[self.store_type]
            self.vector_store = get_vector_store(self.store_type, **config)

            RAGSystem._initialized = True
            print("RAGSystem singleton instance created and initialized.")

    def _get_embedding_fn(self, embedding_model_name: str):
        device = "cuda" if torch.cuda.is_available() else "cpu"
        if embedding_model_name != self.current_embedding_model or self.embedding_fn is None:
            self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name=embedding_model_name,
                device=device,
                normalize_embeddings=True,
                trust_remote_code=True
            )
            self.current_embedding_model = embedding_model_name
        return self.embedding_fn

    def get_or_create_collection(
        self,
        embedding_model_name: str,
        uploaded_files: List[str],
        vector_db_type: str = None,
    ):
        """T·∫°o ho·∫∑c t√°i s·ª≠ d·ª•ng collection d·ª±a tr√™n thay ƒë·ªïi file."""
        # C·∫≠p nh·∫≠t store_type n·∫øu c√≥ thay ƒë·ªïi
        if vector_db_type:
            self.store_type = vector_db_type.lower()
            config = VECTOR_DB_CONFIG[self.store_type]
            self.vector_store = get_vector_store(self.store_type, **config)

        embedding_fn = self._get_embedding_fn(embedding_model_name)
        config = VECTOR_DB_CONFIG[self.store_type]

        # T√≠nh hash c√°c file hi·ªán t·∫°i
        current_hashes: Dict[str, str] = {}
        valid_files = []
        for file_path in (uploaded_files or []):
            if os.path.exists(file_path):
                file_hash = get_file_hash(file_path)
                current_hashes[file_path] = file_hash
                valid_files.append(file_path)
            else:
                print(f"File kh√¥ng t·ªìn t·∫°i (c√≥ th·ªÉ ƒë√£ b·ªã x√≥a): {file_path}")

        print(f"Indexed files (before): {self.indexed_files}")
        print(f"Current file hashes: {current_hashes}")

        # Ki·ªÉm tra c√≥ thay ƒë·ªïi kh√¥ng
        files_changed = False

        # File m·ªõi ho·∫∑c thay ƒë·ªïi n·ªôi dung
        for fp, new_hash in current_hashes.items():
            if self.indexed_files.get(fp) != new_hash:
                files_changed = True
                print(f"File m·ªõi ho·∫∑c ƒë√£ thay ƒë·ªïi: {os.path.basename(fp)}")

        # File b·ªã x√≥a kh·ªèi danh s√°ch
        for old_fp in list(self.indexed_files.keys()):
            if old_fp not in current_hashes:
                files_changed = True
                print(f"File b·ªã x√≥a kh·ªèi danh s√°ch: {os.path.basename(old_fp)}")

        # ƒêi·ªÅu ki·ªán rebuild: l·∫ßn ƒë·∫ßu HO·∫∂C c√≥ thay ƒë·ªïi file
        need_rebuild = files_changed or len(self.indexed_files) == 0

        if need_rebuild:
            print("Rebuild collection (l·∫ßn ƒë·∫ßu ho·∫∑c c√≥ thay ƒë·ªïi file)")
            self.vector_store.delete_collection()

            # T·∫°o collection m·ªõi
            self.vector_store.get_or_create_collection(
                embedding_fn=embedding_fn,
                collection_name=config["collection_name"]
            )

            # Index t√†i li·ªáu
            chunks, ids, metadatas = [], [], []
            for file_path in valid_files:
                filename = os.path.basename(file_path)
                print(f"ƒêang x·ª≠ l√Ω: {filename}")
                text = extract_text(file_path)
                for i, chunk in enumerate(chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)):
                    chunk_id = f"{filename}_chunk_{i:04d}"
                    chunks.append(chunk)
                    ids.append(chunk_id)
                    metadatas.append({"source": file_path})

            if chunks:
                self.vector_store.add_documents(chunks, ids, metadatas)
                print(f"ƒê√£ index {len(chunks)} chunks v√†o {self.store_type.upper()}")

            # C·∫≠p nh·∫≠t tr·∫°ng th√°i
            self.indexed_files = current_hashes.copy()
        else:
            print("Kh√¥ng c√≥ thay ƒë·ªïi ‚Üí Gi·ªØ nguy√™n collection hi·ªán t·∫°i")
            # Quan tr·ªçng: load l·∫°i collection n·∫øu ch∆∞a c√≥ (do restart kernel ho·∫∑c l·∫ßn ƒë·∫ßu kh√¥ng rebuild)
            if self.vector_store.collection is None:
                self.vector_store.get_or_create_collection(
                    embedding_fn=embedding_fn,
                    collection_name=config["collection_name"]
                )

    def retrieve(self, query: str, k: int = 6) -> List[Tuple[str, float, dict]]:
        if not self.vector_store or self.vector_store.count() == 0:
            print("Vector store ch∆∞a s·∫µn s√†ng ho·∫∑c collection r·ªóng.")
            return []

        results = self.vector_store.query(
            query_text=query,
            embedding_fn=self.embedding_fn,
            k=k
        )

        if not results or not results.get("documents") or not results["documents"][0]:
            return []

        return [
            (doc, 1.0 - (dist or 0), meta)
            for doc, dist, meta in zip(
                results["documents"][0],
                results["distances"][0],
                results["metadatas"][0]
            )
        ]

#     def build_prompt(self, query: str, context_items: List[Tuple[str, float, dict]]) -> str:
#         context_text = "\n\n".join([
#             f"[Ngu·ªìn: {os.path.basename(m['source'])}]: {c}"
#             for c, s, m in context_items
#         ])
#         prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh v√† ch√≠nh x√°c. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a CH·ªà v√†o th√¥ng tin ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y.
# N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan trong ng·ªØ c·∫£nh, h√£y tr·∫£ l·ªùi "T√¥i kh√¥ng bi·∫øt" ho·∫∑c "Th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu".

# NG·ªÆ C·∫¢NH:
# {context_text}

# C√ÇU H·ªéI: {query}
# TR·∫¢ L·ªúI:"""
#         return prompt

    def build_prompt(
        self,
        query: str,
        context_items: List[Tuple[str, float, dict]],
        conversation_history: Optional[List[Dict[str, str]]] = None  # Optional history for multi-turn
    ) -> str:
        """
        Build a properly formatted prompt for Gemma 3 IT models.
        
        Args:
            query: Current user question
            context_items: List of (chunk_text, score, metadata)
            conversation_history: Previous messages in OpenAI-style format 
                                 [{"role": "user"/"assistant", "content": "..."}]
        """
        # Build context block with sources
        context_texts = [
            f"[Ngu·ªìn: {os.path.basename(metadata['source'])}]\n{chunk}"
            for chunk, score, metadata in context_items
        ]
        context_block = "\n\n".join(context_texts)

        # System-like instructions (must be in first user turn for Gemma 3)
        system_instruction = (
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh, ch√≠nh x√°c v√† trung th·ª±c. "
            "H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a CH·ªà v√†o th√¥ng tin trong NG·ªÆ C·∫¢NH d∆∞·ªõi ƒë√¢y. "
            "Tr√≠ch d·∫´n ngu·ªìn khi s·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu. "
            "N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y tr·∫£ l·ªùi r√µ r√†ng: "
            "\"T√¥i kh√¥ng bi·∫øt\" ho·∫∑c \"Th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi\"."
        )

        # Start building the formatted prompt
        prompt_parts = []

        # Add conversation history if provided
        if conversation_history:
            for msg in conversation_history:
                if msg["role"] == "user":
                    prompt_parts.append(f"<start_of_turn>user\n{msg['content']}<end_of_turn>\n")
                elif msg["role"] == "assistant":
                    prompt_parts.append(f"<start_of_turn>model\n{msg['content']}<end_of_turn>\n")

        # Add the current turn: combine system instruction + context + query in the user message
        user_message = f"{system_instruction}\n\nNG·ªÆ C·∫¢NH:\n{context_block}\n\nC√ÇU H·ªéI: {query}"

        prompt_parts.append(f"<start_of_turn>user\n{user_message}<end_of_turn>\n")
        
        # Crucial: End with model's turn so it starts generating
        prompt_parts.append("<start_of_turn>model")

        return "".join(prompt_parts)

    def stream_answer(
        self,
        query: str,
        k: int = 6,
        model: str = "llama3",
        params: dict = None
    ) -> Generator[str, None, None]:
        context = self.retrieve(query, k=k)
        prompt = self.build_prompt(query, context)

        options = {
            "temperature": params.get("temperature", 0.7) if params else 0.7,
            "top_k": params.get("top_k", 40) if params else 40,
            "top_p": params.get("top_p", 0.9) if params else 0.9,
            "repeat_penalty": params.get("repeat_penalty", 1.1) if params else 1.1,
        }
        if params and params.get("max_tokens") and params["max_tokens"] > 0:
            options["num_predict"] = params["max_tokens"]

        stream = ollama.chat(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
            options=options,
        )

        for chunk in stream:
            yield chunk["message"]["content"]